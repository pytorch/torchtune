# Copyright (c) Meta Platforms, Inc. and affiliates.
# All rights reserved.
#
# This source code is licensed under the BSD-style license found in the
# LICENSE file in the root directory of this source tree.

# flake8: noqa

# Content of this module fully copied from PyTorch TorchTitan:
# https://github.com/pytorch/torchtitan/blob/main/torchtitan/experiments/kernels/moe/indices.py
# Original author: Less Wright

import torch
import triton
import triton.language as tl


__all__ = ["generate_permute_indices"]


# parallelized kernel
@triton.jit
def _fill_indices_kernel(
    tokens_per_expert_group_ptr,
    start_index_values_ptr,
    write_offsets_ptr,
    output_ptr,
    total_tokens_per_expert_ptr,  # Added to check for zero tokens
    experts_per_rank: tl.constexpr,
    num_ranks: tl.constexpr,
    BLOCK_SIZE: tl.constexpr,  # Number of threads per block
):
    pid = tl.program_id(axis=0)
    num_programs = tl.num_programs(axis=0)

    # map programs (blocks) to the experts and loop (grid stride) if needed
    for expert_id in range(pid, experts_per_rank, num_programs):
        # read this experts write offset
        write_offset = tl.load(write_offsets_ptr + expert_id)

        # get total tokens for this expert across all ranks
        total_expert_tokens = tl.load(total_tokens_per_expert_ptr + expert_id)

        # loop over all ranks, skip if no tokens
        if total_expert_tokens > 0:
            for r in range(num_ranks):
                # index into tokens_per_expert_group array
                i = r * experts_per_rank + expert_id

                # load start index and number of tokens for this expert-rank pair
                start_index = tl.load(start_index_values_ptr + i)
                length = tl.load(tokens_per_expert_group_ptr + i)

                # we can skip this rank-expert pair if there are no tokens
                if length > 0:
                    # each thread in block processes tokens in parallel
                    offsets = tl.arange(0, BLOCK_SIZE)

                    # tokens are processed in chunks of BLOCK_SIZE
                    for chunk_start in range(0, length, BLOCK_SIZE):
                        chunk_offsets = chunk_start + offsets

                        # mask valid indices
                        mask = chunk_offsets < length

                        values = start_index + chunk_offsets

                        # destination
                        dest_indices = write_offset + chunk_offsets

                        # store
                        tl.store(output_ptr + dest_indices, values, mask=mask)

                    # update write offset for next rank
                    write_offset += length


# ==============
# wrapper
# ==============


def fill_indices_wrapper(
    tokens_per_expert_group: torch.Tensor,
    start_index_values: torch.Tensor,
    write_offsets: torch.Tensor,
    total_tokens_per_expert: torch.Tensor,
    experts_per_rank: int,
    num_ranks: int,
    max_len: int,
    block_size: int = 128,
    max_blocks: int = 1024,  # cap on total number of blocks to launch
):
    # preallocate output
    permuted_indices = torch.full(
        (max_len,), -1, dtype=torch.int32, device=tokens_per_expert_group.device
    )

    # write offsets is per local expert...
    num_blocks = min(experts_per_rank, max_blocks)
    # grid = one block per expert unless capped and then we loop...
    grid = (num_blocks,)

    # launch kernel
    _fill_indices_kernel[grid](
        tokens_per_expert_group,
        start_index_values,
        write_offsets,
        permuted_indices,
        total_tokens_per_expert,  # 'skip logic' check for zero tokens
        experts_per_rank,
        num_ranks,
        BLOCK_SIZE=block_size,
    )
    return permuted_indices


# reference
def fill_indices_cpu(
    tokens_per_expert_group: torch.Tensor,
    start_index_values: torch.Tensor,
    write_offsets: torch.Tensor,
    total_tokens_per_expert: torch.Tensor,
    experts_per_rank: int,
    num_ranks: int,
    max_len: int,
):
    # We need to preallocate the output - we ignore device and force it on cpu
    # device = tokens_per_expert_group.device
    permuted_indices = torch.full(
        (max_len,),
        -1,
        dtype=torch.int32,
    )  # device=device)
    # Fill the permuted indices
    # For each local expert
    for e in range(experts_per_rank):
        write_start = write_offsets[e].item()
        total_tokens = total_tokens_per_expert[e].item()

        # For each remote rank
        # we can skip this expert if it has no tokens, already filled with -1
        if total_tokens > 0:
            for r in range(num_ranks):
                i = r * experts_per_rank + e
                start_index = start_index_values[i].item()
                length = tokens_per_expert_group[i].item()
                # Fill in the indices
                if length > 0:
                    end_idx = min(write_start + length, max_len)
                    permuted_indices[write_start:end_idx] = torch.arange(
                        start_index,
                        start_index + (end_idx - write_start),
                        dtype=torch.int32,
                        # device=device,
                    )
                write_start += length
    return permuted_indices


def generate_permute_indices(
    tokens_per_expert_group: torch.Tensor,
    experts_per_rank: int,
    num_ranks: int,
    max_len: int,
    alignment: int,
    use_cpu: bool = False,
):
    """
    Prepare permutation indices and the number of tokens for each expert.

    Args:
        tokens_per_expert_group (torch.Tensor): number of tokens for each expert from all ranks.
        experts_per_rank (int): number of experts per rank.
        num_ranks (int): number of ranks.
        max_len (int): maximum length of the output index vector.
        alignment (int): alignment for each returned element in `m_sizes` and padding min for zero token experts.
        use_cpu (bool): whether to use CPU implementation.


    Returns:
        permuted_indices: Tensor of indices that map original token order to the expert-grouped order.
        m_sizes: aligned number of tokens for each expert (padded to alignment boundary).
        m_offsets: Cumulative sum of m_sizes. The exclusive ending position for each expert's tokens.

    Explanatory details:
        `tokens_per_expert_group` is of shape (num_ranks * experts_per_rank,), for example:
        From: |       rank 0      |       rank 1      |
        To:   | E0 | E1 | E2 | E3 | E0 | E1 | E2 | E3 |
              |  4 |  2 |  1 |  3 |  1 |  2 |  3 |  4 |
    """

    # prefix sum to get start index of each expert (parallel scan kernel in future?)
    start_index_values = (
        torch.cumsum(tokens_per_expert_group, 0) - tokens_per_expert_group
    )

    # total tokens for each expert (sum over ranks)
    total_tokens_per_expert = tokens_per_expert_group.view(num_ranks, -1).sum(0)

    # pad out empty experts to alignment requirement
    padded_total_tokens_per_expert = torch.clamp_min(total_tokens_per_expert, alignment)

    # align the chunk sizes (cdiv)
    m_sizes = (
        (padded_total_tokens_per_expert + alignment - 1) // alignment * alignment
    ).to(torch.int32)

    # additional prefix sum to get write offset of each expert in permuted_indices
    # write offsets is per local expert, not global
    m_offsets = torch.cumsum(m_sizes, 0)
    write_offsets = m_offsets - m_sizes

    # Select the implementation to use
    if use_cpu:
        permuted_indices = fill_indices_cpu(
            tokens_per_expert_group,
            start_index_values,
            write_offsets,
            total_tokens_per_expert,  # Pass to check for zero tokens
            experts_per_rank,
            num_ranks,
            max_len,
        )
    else:
        permuted_indices = fill_indices_wrapper(
            tokens_per_expert_group,
            start_index_values,
            write_offsets,
            total_tokens_per_expert,  # Pass to check for zero tokens
            experts_per_rank,
            num_ranks,
            max_len,
        )

    return permuted_indices, m_sizes, m_offsets.to(torch.int32)
