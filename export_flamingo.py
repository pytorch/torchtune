import os, sys
import time
import warnings
from functools import lru_cache, wraps
from typing import Optional

import numpy as np
import PIL
import torch
from executorch import exir
from executorch.exir.passes.sym_shape_eval_pass import ConstraintBasedSymShapeEvalPass

from torch._inductor.package import package_aoti
from torchtune.data import Message, padded_collate_tiled_images_and_mask
from torchtune.data._prompt_templates import _TemplateType
from torchtune.generation._generation import sample
from torchtune.models.clip._transform import CLIPImageTransform
from torchtune.models.flamingo import (
    flamingo_decoder,
    flamingo_vision_encoder,
    FlamingoTransform,
)

from torchtune.models.flamingo._component_builders import (
    flamingo_decoder,
    flamingo_vision_encoder,
)
from torchtune.modules.model_fusion import DeepFusionModel
from torchvision.transforms.v2 import functional as F

max_seq_len = 8192
in_channels = 3
tile_size = 560
max_num_tiles = 4
# how many tokens per image generated by the vision encoder
tokens_per_image = 6404
# how many images to cache in the kv cache in cross attention
kv_cache_image_num = 1
# maximum number of tokens generated by encoder and thus stored in the kv cache in cross attention
encoder_max_seq_len = tokens_per_image * kv_cache_image_num


@lru_cache(maxsize=1)
def get_vision_encoder():
    return flamingo_vision_encoder(
        patch_size=14,
        num_heads=16,
        clip_embed_dim=1280,
        clip_num_layers=32,
        clip_hidden_states=[3, 7, 15, 23, 30],
        decoder_embed_dim=4096,
        num_layers_projection=8,
        tile_size=tile_size,
        max_num_tiles=4,
        in_channels=3,
    )


@lru_cache(maxsize=1)
def get_text_decoder():
    return flamingo_decoder(
        vocab_size=128_256,
        num_layers=32,
        fusion_interval=4,
        num_special_tokens=8,
        num_heads=32,
        num_kv_heads=8,
        embed_dim=4096,
        max_seq_len=max_seq_len,
        encoder_max_seq_len=encoder_max_seq_len,
        rope_base=500000.0,
        intermediate_dim=14336,
    )


@lru_cache(maxsize=1)
def get_flamingo(llama3_2_dir):
    model = DeepFusionModel(
        encoder=get_vision_encoder(),
        decoder=get_text_decoder(),
        encoder_trainable=False,
        decoder_trainable=False,
        fusion_trainable=False,
    )
    print("Load checkpoint")
    state_dict = torch.load(os.path.join(llama3_2_dir, "tune.pth"))

    model.load_state_dict(state_dict)
    model.setup_caches(
        batch_size=1,
        dtype=torch.float32,
        encoder_max_seq_len=encoder_max_seq_len,  # Hardcoded in for now
        decoder_max_seq_len=max_seq_len,
    )
    return model


def flamingo_transform(
    path: str,
    max_seq_len: int = 8192,
    special_tokens_path: Optional[str] = None,
    prompt_template: Optional[_TemplateType] = None,
) -> FlamingoTransform:
    """
    Data Transforms (including Tokenizer) for Llama3 Vision.

    Args:
        path (str): path to the tokenizer
        max_seq_len (int): maximum sequence length for tokenizing a single list of messages,
            after which the input will be truncated.
        special_tokens_path (Optional[str]): Path to ``tokenizer.json`` from Hugging Face
            model files that contains all registered special tokens, or a local json file
            structured similarly. Default is None to use the canonical Llama3 special tokens.
        prompt_template (Optional[_TemplateType]): optional specified prompt template.
            If a string, it is assumed to be the dotpath of a :class:`~torchtune.data.PromptTemplateInterface`
            class. If a dictionary, it is assumed to be a custom prompt template mapping role to the
            prepend/append tags.

    Returns:
        FlamingoTransform: Instantiation of the Llama3 tokenizer
    """
    special_tokens = (
        parse_hf_tokenizer_json(special_tokens_path)
        if special_tokens_path is not None
        else None
    )
    template = (
        _get_prompt_template(prompt_template) if prompt_template is not None else None
    )
    return FlamingoTransform(
        path=path,
        special_tokens=special_tokens,
        tile_size=560,
        patch_size=14,
        max_num_tiles=4,
        max_seq_len=max_seq_len,
        image_mean=(0.48145466, 0.4578275, 0.40821073),
        image_std=(0.26862954, 0.26130258, 0.27577711),
        prompt_template=template,
    )


@lru_cache(maxsize=1)
def get_sample_preprocess_outputs(llama3_2_dir):
    image_path = os.path.join(llama3_2_dir, "dog.jpg")
    tokenizer_path = os.path.join(llama3_2_dir, "tokenizer.model")
    transform = flamingo_transform(tokenizer_path)
    images = [PIL.Image.open(image_path)]
    messages = [
        Message(
            role="user",
            content=[
                {"type": "image", "content": images[0]},
                {"type": "text", "content": "What's in this image?"},
            ],
            eot=True,
        ),
        Message(role="assistant", content=""),
    ]
    data = transform({"messages": messages}, inference=True)
    seq_len = len(data["tokens"])
    total_response_length = max_seq_len
    # mask and input_pos
    causal_mask = torch.tril(
        torch.ones(
            size=(total_response_length, total_response_length),
            dtype=torch.bool,
        )
    )
    input_pos = torch.arange(total_response_length)
    batch = padded_collate_tiled_images_and_mask(
        [data], pad_direction="left", pad_max_images=kv_cache_image_num
    )
    batch["mask"] = causal_mask[None, :seq_len, :]
    batch["input_pos"] = input_pos[None, :seq_len]
    batch["encoder_mask"] = batch["encoder_mask"][:, :seq_len]
    return batch


@lru_cache(maxsize=1)
def get_text_decoder_inputs(llama3_2_dir, model):
    data = get_sample_preprocess_outputs(llama3_2_dir).copy()
    # breakpoint()
    image = data["encoder_input"]["images"].to(dtype=torch.float32)
    embeds = model.encoder(image, data["encoder_input"]["aspect_ratio"])
    data["encoder_input"] = embeds
    tokens = data.pop("tokens")
    return tokens, data


@lru_cache(maxsize=1)
def get_vision_encoder_dynamic_shapes():
    dim = torch.export.Dim("num_tiles", min=1, max=max_num_tiles)
    image_dynamic_dim = {
        0: 1,
        1: 1,
        2: dim,
        3: 3,
        4: tile_size,
        5: tile_size,
    }
    return image_dynamic_dim


@lru_cache(maxsize=1)
def get_text_decoder_dynamic_shapes():
    dim = torch.export.Dim("token_dim", min=1, max=max_seq_len)
    dim_enc = torch.export.Dim("enc_dim", min=1, max=encoder_max_seq_len)

    return {
        "tokens": {0: 1, 1: dim},
        "mask": {0: 1, 1: dim, 2: None},
        "encoder_input": None,
        "encoder_mask": {0: 1, 1: dim, 2: None},
        "input_pos": {0: 1, 1: dim},
    }


def timeit(func):
    @wraps(func)
    def wrapper(*args, **kwargs):
        start_time = time.time()
        result = func(*args, **kwargs)
        end_time = time.time()
        print(
            f"Function {func.__name__} took {end_time - start_time} seconds to execute."
        )
        return result

    return wrapper


@timeit
def run_vision_encoder_eager(vision_encoder, image, aspect_ratio):
    return vision_encoder(image, aspect_ratio)


def benchmark_vision_encoder(vision_encoder, image, aspect_ratio):
    # warm up run
    vision_encoder(image, aspect_ratio)
    # time it
    total_time = 0
    for _ in range(30):
        start_time = time.time()
        res = vision_encoder(image, aspect_ratio)
        total_time += time.time() - start_time
    return total_time / 30, res


def benchmark_all_vision_encoder(llama3_2_dir):
    preprocess_outputs = get_sample_preprocess_outputs(llama3_2_dir)
    image = preprocess_outputs["image"]
    # Eager
    aspect_ratio = preprocess_outputs["aspect_ratio"]
    print("image shape:", image.shape)
    for dtype in [torch.bfloat16, torch.float32]:
        image = image.to(dtype=dtype)
        aspect_ratio = aspect_ratio.to(dtype=dtype)
        vision_encoder = get_vision_encoder().to(dtype=dtype).eval()
        print(
            f"-----------------------------------Eager Mode {dtype} CPU-----------------------------------"
        )
        avg, eager_res = benchmark_vision_encoder(vision_encoder, image, aspect_ratio)
        print(f"Averaged time: {avg}")

        # # Torch.compile
        # print(f"-----------------------------------Torch.compile {dtype} CPU-----------------------------------")
        # with torch.no_grad():
        #     compiled_vision_encoder = torch.compile(vision_encoder, mode="reduce-overhead")
        # # warm up run
        # compiled_vision_encoder(image, aspect_ratio)
        # # time it
        # avg, compiled_res = benchmark_vision_encoder(compiled_vision_encoder, image, aspect_ratio)
        # print(f"Averaged time: {avg}")
        # print(f"Close to eager? {torch.allclose(eager_res, compiled_res)}")

        # # torch.export
        # print(f"-----------------------------------Torch.export {dtype} CPU-----------------------------------")
        image_dynamic_dim = get_vision_encoder_dynamic_shapes()
        # ep = torch.export.export(
        #     vision_encoder,
        #     (image, aspect_ratio),
        #     dynamic_shapes=(image_dynamic_dim, None),
        # )
        # avg, exported_res = benchmark_vision_encoder(ep.module(), image, aspect_ratio)
        # print(f"Averaged time: {avg}")
        # print(f"Close to eager? {torch.allclose(eager_res, exported_res)}")

        # AOTInductor
        print(
            f"-----------------------------------AOTInductor {dtype} CPU-----------------------------------"
        )
        with torch.nn.attention.sdpa_kernel([torch.nn.attention.SDPBackend.MATH]):
            so = torch._export.aot_compile(
                vision_encoder,
                args=(image, aspect_ratio),
                options={"aot_inductor.output_path": "/tmp/vision_encoder.so"},
                dynamic_shapes=(image_dynamic_dim, None),
            )
        aot_loaded = torch._export.aot_load(so, device="cpu")
        avg, aoti_res = benchmark_vision_encoder(aot_loaded, image, aspect_ratio)
        print(f"Averaged time: {avg}")
        print(f"Close to eager? {torch.allclose(eager_res, aoti_res)}")
        print(
            f"-----------------------------------Eager Mode {dtype} GPU-----------------------------------"
        )
        image_cuda = image.to(device="cuda")
        aspect_ratio_cuda = aspect_ratio.to(device="cuda")
        vision_encoder_cuda = vision_encoder.to(device="cuda")
        avg, eager_res_cuda = benchmark_vision_encoder(
            vision_encoder_cuda, image_cuda, aspect_ratio_cuda
        )
        print(f"Averaged time: {avg}")
        print(f"Close to eager? {torch.allclose(eager_res, eager_res_cuda.cpu())}")
        # Torch.compile
        # print("-----------------------------------Torch.compile fp32 GPU-----------------------------------")
        # with torch.no_grad():
        #     compiled_vision_encoder_cuda = torch.compile(vision_encoder_cuda, mode="reduce-overhead")
        # # warm up run
        # compiled_vision_encoder_cuda(image_cuda, aspect_ratio_cuda)
        # # time it
        # compiled_res = run_vision_encoder_eager(compiled_vision_encoder_cuda, image_cuda, aspect_ratio_cuda)
        # print(f"Close to eager? {torch.allclose(eager_res, compiled_res)}")

        # print("-----------------------------------AOTInductor fp32 GPU-----------------------------------")
        # with torch.nn.attention.sdpa_kernel([torch.nn.attention.SDPBackend.MATH]):
        #     so = torch._export.aot_compile(
        #         vision_encoder_cuda,
        #         args=(image_cuda, aspect_ratio_cuda),
        #         options={"aot_inductor.output_path": "/tmp/vision_transformer.so"},
        #         dynamic_shapes=(image_dynamic_dim, None),
        #     )
        # aoti_res = run_vision_encoder_eager(torch._export.aot_load(so, device="cuda"), image_cuda, aspect_ratio_cuda)


def export_vision_encoder(llama3_2_dir):
    preprocess_outputs = get_sample_preprocess_outputs(llama3_2_dir)
    image = preprocess_outputs["encoder_input"]["image"]
    aspect_ratio = preprocess_outputs["encoder_input"]["aspect_ratio"]
    image_dynamic_dim = get_vision_encoder_dynamic_shapes()
    vision_encoder = get_vision_encoder().eval()
    with torch.no_grad():
        print("Start to export vision encoder")
        with torch.nn.attention.sdpa_kernel([torch.nn.attention.SDPBackend.MATH]):
            ep = torch.export.export(
                vision_encoder,
                (image, aspect_ratio),
                dynamic_shapes=(image_dynamic_dim, None),
            )
        print("Done exporting vision encoder")
    return ep


def aoti_export_vision_encoder(llama3_2_dir):
    preprocess_outputs = get_sample_preprocess_outputs(llama3_2_dir)
    image = preprocess_outputs["encoder_input"]["images"].to(dtype=torch.float32)
    aspect_ratio = preprocess_outputs["encoder_input"]["aspect_ratio"]
    image_dynamic_dim = get_vision_encoder_dynamic_shapes()
    breakpoint()
    model = get_flamingo(llama3_2_dir)

    print("Start to AOTI export vision encoder")
    with torch.nn.attention.sdpa_kernel([torch.nn.attention.SDPBackend.MATH]):
        so = torch._export.aot_compile(
            model.encoder,
            args=(image, aspect_ratio),
            options={"aot_inductor.package": True},
            dynamic_shapes=(image_dynamic_dim, None),
        )
        package_aoti(os.path.join(llama3_2_dir, "vision_encoder.pt2"), so)
    print("Done AOTI exporting vision encoder")


def aoti_export_text_decoder(llama3_2_dir):
    """
    (Pdb) encoder_embed.shape
    torch.Size([1, 6404, 4096])
    (Pdb) mask.shape
    torch.Size([1, 17, 117])
    (Pdb) encoder_mask.shape
    torch.Size([1, 17, 6404])
    (Pdb) input_pos.shape
    torch.Size([1, 17])
    (Pdb) tokens.shape
    torch.Size([1, 17])
    """
    model = get_flamingo(llama3_2_dir)

    with torch.no_grad():
        dynamic_shapes = get_text_decoder_dynamic_shapes()
        tokens, data = get_text_decoder_inputs(llama3_2_dir, model)

        print("Start to generate aoti for text decoder")
        with torch.nn.attention.sdpa_kernel([torch.nn.attention.SDPBackend.MATH]):
            so = torch._export.aot_compile(
                model.decoder,
                (tokens,),
                data,
                options={"aot_inductor.package": True},
                dynamic_shapes=dynamic_shapes,
            )
            package_aoti(os.path.join(llama3_2_dir, "text_decoder.pt2"), so)


def export_text_decoder(llama3_2_dir):
    model = get_flamingo(llama3_2_dir)

    class Decoder(torch.nn.Module):
        def __init__(self, model) -> None:
            super().__init__()
            self.decoder = model.decoder

        def forward(
            self,
            tokens: torch.Tensor,
            *,
            mask: Optional[torch.Tensor] = None,
            encoder_input: Optional[torch.Tensor] = None,
            encoder_mask: Optional[torch.Tensor] = None,
            input_pos: Optional[torch.Tensor] = None,
        ):
            return self.decoder(
                tokens,
                mask=mask,
                encoder_input=encoder_input,
                encoder_mask=encoder_mask,
                input_pos=input_pos,
            )

    m = Decoder(model)
    with torch.no_grad():
        dynamic_shapes = get_text_decoder_dynamic_shapes()
        tokens, data = get_text_decoder_inputs(llama3_2_dir, model)

        print("Start to export text decoder")
        with torch.nn.attention.sdpa_kernel([torch.nn.attention.SDPBackend.MATH]):
            ep = torch.export.export(
                m,
                (tokens,),
                data,
                dynamic_shapes=dynamic_shapes,
            )
            print(ep)
    return ep


def get_random_input_to_decoder():
    encoder_input = torch.randn(1, 6404, 4096)
    seq_len = 17
    causal_mask = torch.tril(
        torch.ones(
            size=(max_seq_len, max_seq_len),
            dtype=torch.bool,
        )
    )
    mask = causal_mask[None, :seq_len, :]
    encoder_mask = torch.ones(1, seq_len, 6404, dtype=torch.bool)
    input_pos = torch.arange(seq_len).unsqueeze(0)
    tokens = torch.tensor(
        [
            [
                128000,
                128006,
                882,
                128007,
                271,
                128256,
                3923,
                596,
                304,
                420,
                2217,
                30,
                128009,
                128006,
                78191,
                128007,
                271,
            ]
        ]
    )

    return tokens, {
        "encoder_input": encoder_input,
        "mask": mask,
        "encoder_mask": encoder_mask,
        "input_pos": input_pos,
    }


def validate_text_decoder(llama3_2_dir):
    # aoti_export_text_decoder(llama3_2_dir)
    # compare result
    aoti = torch._inductor.aoti_load_package(
        os.path.join(llama3_2_dir, "text_decoder.pt2")
    )
    print("AOTI loaded")

    # export = export_text_decoder(llama3_2_dir)

    # inputs
    # tokens, data = get_text_decoder_inputs(llama3_2_dir, model)
    tokens, data = get_random_input_to_decoder()
    print("Start running aoti")
    # run eager
    # breakpoint()
    aoti_res = aoti(tokens, **data)

    model = get_flamingo(llama3_2_dir)
    eager = model.decoder
    eager_res = eager(tokens, **data)
    # export_res = export.module()(tokens, **data)

    # debug
    # from torch._inductor.decomposition import select_decomp_table
    # inductor_decomp_gm = export.run_decompositions(select_decomp_table()).module()

    # from torch._inductor.compile_fx import _recursive_post_grad_passes
    # from torch._export.utils import _detect_fake_mode_from_gm
    # from torch._inductor.virtualized import V
    # fake_mode = _detect_fake_mode_from_gm(inductor_decomp_gm)
    # with V.set_fake_mode(fake_mode):
    #     _recursive_post_grad_passes(inductor_decomp_gm, is_inference=True)

    # debug_res = inductor_decomp_gm(tokens, encoder_input=encoder_input, input_pos=input_pos)

    print(
        f"AOTI close to eager? {torch.allclose(eager_res, aoti_res, atol=1e-3, rtol=1e-3)}"
    )
    # print(f"Export close to eager? {torch.allclose(eager_res, export_res)}")

    print(f"Eager result: {eager_res}")
    # print(f"Export result: {export_res}")
    # print(f"Debug result: {debug_res}")
    print(f"AOTInductor result: {aoti_res}")


def validate_vision_encoder(llama3_2_dir):
    preprocess_outputs = get_sample_preprocess_outputs(llama3_2_dir)
    image = preprocess_outputs["encoder_input"]["images"].to(dtype=torch.float32)
    aspect_ratio = preprocess_outputs["encoder_input"]["aspect_ratio"]
    # eager model
    vision_encoder = get_vision_encoder().eval()
    # aoti export
    print("Start to load vision_encoder.pt2")
    aoti = torch._inductor.aoti_load_package(
        os.path.join(llama3_2_dir, "vision_encoder.pt2")
    )
    print("Finished to load vision_encoder.pt2")

    # export
    # export = export_vision_encoder()
    # results
    # aoti_res = aoti(image, aspect_ratio)
    # print("Finished aoti run, running eager")
    # print(f"AOTInductor result: {aoti_res}")

    eager_res = vision_encoder(image, aspect_ratio)
    print(f"Eager result: {eager_res}")

    # export_res = export.module()(image, aspect_ratio)
    print(
        f"AOTI close to eager? {torch.allclose(eager_res, aoti_res, atol=1e-3, rtol=1e-3)}"
    )
    # print(f"Export close to eager? {torch.allclose(eager_res, export_res)}")

    # print(f"Export result: {export_res}")


def test_aoti(llama3_2_dir):
    aoti = torch._inductor.aoti_load_package(
        os.path.join(llama3_2_dir, "text_decoder.pt2")
    )
    model = get_flamingo(llama3_2_dir)
    eager = model.decoder
    # export = export_text_decoder(llama3_2_dir)
    transform = flamingo_transform(os.path.join(llama3_2_dir, "tokenizer.model"))
    # inputs
    tokens, data = get_text_decoder_inputs(llama3_2_dir, model)
    seq_len = tokens.shape[1]

    # first run with image
    print("First run with image")
    eager_res = eager(tokens, **data)
    aoti_res = aoti(tokens, **data)
    print(
        f"AOTI close to eager? {torch.allclose(eager_res, aoti_res, atol=1e-3, rtol=1e-3)}"
    )

    # second run with no image
    print("Second run with no image")
    tok = sample(aoti_res[:, -1]).to(torch.int64)
    generated_tokens = [tok]

    input_pos = torch.arange(max_seq_len)
    causal_mask = torch.tril(
        torch.ones(
            size=(max_seq_len, max_seq_len),
            dtype=torch.bool,
        )
    )
    encoder_input = data.pop("encoder_input")
    for _ in range(10):
        # adjust input
        data["encoder_mask"] = data["encoder_mask"][:, -1:]
        data["input_pos"] = input_pos[None, seq_len]
        data["mask"] = causal_mask[None, seq_len, None, :]
        # run
        aoti_logits = aoti(
            tok,
            encoder_input=torch.full_like(encoder_input, torch.nan),
            encoder_mask=data["encoder_mask"],
            mask=data["mask"],
            input_pos=data["input_pos"],
        )
        eager_logits = eager(
            tok,
            encoder_input=torch.full_like(encoder_input, torch.nan),
            encoder_mask=data["encoder_mask"],
            mask=data["mask"],
            input_pos=data["input_pos"],
        )
        print(
            f"AOTI close to eager? {torch.allclose(eager_logits, aoti_logits, atol=1e-3, rtol=1e-3)}"
        )
        seq_len += 1
        tok = sample(aoti_logits[:, -1]).to(torch.int64)
        generated_tokens.append(tok)
    print(transform.decode(generated_tokens))


if __name__ == "__main__":
    llama3_2_dir = str(sys.argv[1])
    # aoti_export_vision_encoder(llama3_2_dir)
    validate_vision_encoder(llama3_2_dir)
    # aoti_export_text_decoder(llama3_2_dir)
    # test_aoti(llama3_2_dir)
    # validate_text_decoder(llama3_2_dir)
    # model = get_flamingo(llama3_2_dir)

    # with torch.no_grad():

    #     data = get_sample_preprocess_outputs(llama3_2_dir).copy()
    #     image = data["encoder_input"]["images"].to(dtype=torch.float32)
    #     embeds = model.encoder(image, data["encoder_input"]["aspect_ratio"])
    #     data["encoder_input"] = embeds
    #     tokens = data.pop("tokens")
    #     print("Start eager run")
    #     # model.decoder(tokens, **data)
    #     torch._dynamo.config.capture_dynamic_output_shape_ops = True
    #     torch._dynamo.config.capture_scalar_outputs = True
    #     torch.compile(model.decoder, fullgraph=True)(tokens, **data)
