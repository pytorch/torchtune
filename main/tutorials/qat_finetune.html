


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta name="robots" content="noindex">
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Finetuning Llama3 with QAT &mdash; torchtune main documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="../_static/sg_gallery.css" type="text/css" />
  <link rel="stylesheet" href="../_static/sg_gallery-binder.css" type="text/css" />
  <link rel="stylesheet" href="../_static/sg_gallery-dataframe.css" type="text/css" />
  <link rel="stylesheet" href="../_static/sg_gallery-rendered-html.css" type="text/css" />
  <link rel="stylesheet" href="../_static/tabs.css" type="text/css" />
  <link rel="stylesheet" href="../_static/sphinx-design.5ea377869091fd0449014c60fc090103.min.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/custom_torchtune.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="End-to-End Workflow with torchtune" href="e2e_flow.html" />
    <link rel="prev" title="Finetuning Llama2 with QLoRA" href="qlora_finetune.html" />
  <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-T8XT4PS');</script>
    <!-- End Google Tag Manager -->
  

  
  <script src="../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Learn
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/get-started">
                  <span class=dropdown-title>Get Started</span>
                  <p>Run PyTorch locally or get started quickly with one of the supported cloud platforms</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials">
                  <span class="dropdown-title">Tutorials</span>
                  <p>Whats new in PyTorch tutorials</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/basics/intro.html">
                  <span class="dropdown-title">Learn the Basics</span>
                  <p>Familiarize yourself with PyTorch concepts and modules</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/recipes/recipes_index.html">
                  <span class="dropdown-title">PyTorch Recipes</span>
                  <p>Bite-size, ready-to-deploy PyTorch code examples</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/introyt.html">
                  <span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
                  <p>Master PyTorch basics with our engaging YouTube tutorial series</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Ecosystem
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem">
                  <span class="dropdown-title">Tools</span>
                  <p>Learn about the tools and frameworks in the PyTorch Ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class=dropdown-title>Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class=dropdown-title>Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class=dropdown-title>Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem/contributor-awards-2023">
                  <span class="dropdown-title">Contributor Awards - 2023</span>
                  <p>Award winners announced at this year's PyTorch Conference</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Edge
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/edge">
                  <span class="dropdown-title">About PyTorch Edge</span>
                  <p>Build innovative and privacy-aware AI experiences for edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/executorch-overview">
                  <span class="dropdown-title">ExecuTorch</span>
                  <p>End-to-end solution for enabling on-device inference capabilities across mobile and edge devices</p>
                </a>
              </div>
            </div>  
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p>Explore the documentation for comprehensive guidance on how to use PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/pytorch-domains">
                  <span class="dropdown-title">PyTorch Domains</span>
                  <p>Read the PyTorch Domains documentation to learn more about domain-specific libraries</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Blogs & News 
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/blog/">
                  <span class="dropdown-title">PyTorch Blog</span>
                  <p>Catch up on the latest technical news and happenings</p>
                </a>
                 <a class="nav-dropdown-item" href="https://pytorch.org/community-blog">
                  <span class="dropdown-title">Community Blog</span>
                  <p>Stories from the PyTorch ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/videos">
                  <span class="dropdown-title">Videos</span>
                  <p>Learn about the latest PyTorch tutorials, new, and more </p>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                About
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn more about the PyTorch Foundation</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/governing-board">
                  <span class="dropdown-title">Governing Board</span>
                  <p></p>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div class="no-dropdown">
              <a href="https://pytorch.org/join" data-cta="join">
                Become a Member
              </a>
            </div>
          </li>
          <li>
           <div class="main-menu-item">
             <a href="https://github.com/pytorch/pytorch" class="github-icon">
             </a>
           </div>
          </li>
          <!--- TODO: This block adds the search icon to the nav bar. We will enable it later. 
          <li>
            <div class="main-menu-item">
             <a href="https://github.com/pytorch/pytorch" class="search-icon">
             </a>
            </div>
          </li>
          --->
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
        <a href='https://pytorch.org/torchtune/versions.html'>main &#x25BC</a>
      </div>
    


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          </div>

          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../overview.html">torchtune Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install.html">Install Instructions</a></li>
<li class="toctree-l1"><a class="reference internal" href="first_finetune_tutorial.html">Fine-Tune Your First LLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tune_cli.html">torchtune CLI</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Tutorials</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="llama3.html">Meta Llama3 in torchtune</a></li>
<li class="toctree-l1"><a class="reference internal" href="lora_finetune.html">Finetuning Llama2 with LoRA</a></li>
<li class="toctree-l1"><a class="reference internal" href="qlora_finetune.html">Finetuning Llama2 with QLoRA</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Finetuning Llama3 with QAT</a></li>
<li class="toctree-l1"><a class="reference internal" href="e2e_flow.html">End-to-End Workflow with torchtune</a></li>
<li class="toctree-l1"><a class="reference internal" href="datasets.html">Configuring Datasets for Fine-Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="chat.html">Fine-tuning Llama3 with Chat Data</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Deep-Dives</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../deep_dives/checkpointer.html">Checkpointing in torchtune</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deep_dives/configs.html">All About Configs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deep_dives/recipe_deepdive.html">What Are Recipes?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deep_dives/wandb_logging.html">Logging to Weights &amp; Biases</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../api_ref_config.html">torchtune.config</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_ref_data.html">torchtune.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_ref_datasets.html">torchtune.datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_ref_models.html">torchtune.models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_ref_modules.html">torchtune.modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_ref_utilities.html">torchtune.utils</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
      <li>Finetuning Llama3 with QAT</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="../_sources/tutorials/qat_finetune.rst.txt" rel="nofollow"><img src="../_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          <!-- Google Tag Manager (noscript) -->
          <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS"
          height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
          <!-- End Google Tag Manager (noscript) -->
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <section id="finetuning-llama3-with-qat">
<span id="qat-finetune-label"></span><h1>Finetuning Llama3 with QAT<a class="headerlink" href="#finetuning-llama3-with-qat" title="Permalink to this heading">¶</a></h1>
<p>Quantization-Aware Training (QAT) is a common technique for users to quantize their
models without incurring significant degradations in accuracy or perplexity. In this
tutorial, we’ll walk through how to apply QAT during fine-tuning, quantize the
resulting model, and evaluate your quantized model using torchtune.</p>
<div class="sd-container-fluid sd-sphinx-override sd-mb-4 docutils">
<div class="sd-row sd-row-cols-2 sd-row-cols-xs-2 sd-row-cols-sm-2 sd-row-cols-md-2 sd-row-cols-lg-2 docutils">
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm docutils">
<div class="sd-card-body docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
<svg version="1.1" width="1.0em" height="1.0em" class="sd-octicon sd-octicon-mortar-board" viewBox="0 0 16 16" aria-hidden="true"><path d="M7.693 1.066a.747.747 0 0 1 .614 0l7.25 3.25a.75.75 0 0 1 0 1.368L13 6.831v2.794c0 1.024-.81 1.749-1.66 2.173-.893.447-2.075.702-3.34.702-.278 0-.55-.012-.816-.036a.75.75 0 0 1 .133-1.494c.22.02.45.03.683.03 1.082 0 2.025-.221 2.67-.543.69-.345.83-.682.83-.832V7.503L8.307 8.934a.747.747 0 0 1-.614 0L4 7.28v1.663c.296.105.575.275.812.512.438.438.688 1.059.688 1.796v3a.75.75 0 0 1-.75.75h-3a.75.75 0 0 1-.75-.75v-3c0-.737.25-1.358.688-1.796.237-.237.516-.407.812-.512V6.606L.443 5.684a.75.75 0 0 1 0-1.368ZM2.583 5 8 7.428 13.416 5 8 2.572ZM2.5 11.25v2.25H4v-2.25c0-.388-.125-.611-.25-.735a.697.697 0 0 0-.5-.203.707.707 0 0 0-.5.203c-.125.124-.25.347-.25.735Z"></path></svg> What you will learn</div>
<ul class="simple">
<li><p class="sd-card-text">What QAT is and how it helps reduce quantization degradation</p></li>
<li><p class="sd-card-text">How to run QAT during fine-tuning in torchtune</p></li>
<li><p class="sd-card-text">End-to-end example of connecting QAT, quantization, and evaluation recipes</p></li>
</ul>
</div>
</div>
</div>
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm docutils">
<div class="sd-card-body docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
<svg version="1.1" width="1.0em" height="1.0em" class="sd-octicon sd-octicon-list-unordered" viewBox="0 0 16 16" aria-hidden="true"><path d="M5.75 2.5h8.5a.75.75 0 0 1 0 1.5h-8.5a.75.75 0 0 1 0-1.5Zm0 5h8.5a.75.75 0 0 1 0 1.5h-8.5a.75.75 0 0 1 0-1.5Zm0 5h8.5a.75.75 0 0 1 0 1.5h-8.5a.75.75 0 0 1 0-1.5ZM2 14a1 1 0 1 1 0-2 1 1 0 0 1 0 2Zm1-6a1 1 0 1 1-2 0 1 1 0 0 1 2 0ZM2 4a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z"></path></svg> Prerequisites</div>
<ul class="simple">
<li><p class="sd-card-text">Be familiar with <a class="reference internal" href="../overview.html#overview-label"><span class="std std-ref">torchtune</span></a></p></li>
<li><p class="sd-card-text">Make sure to <a class="reference internal" href="../install.html#install-label"><span class="std std-ref">install torchtune</span></a></p></li>
<li><p class="sd-card-text">Make sure you have downloaded the <a class="reference internal" href="first_finetune_tutorial.html#download-llama-label"><span class="std std-ref">Llama3-8B model weights</span></a></p></li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section id="what-is-qat">
<h2>What is QAT?<a class="headerlink" href="#what-is-qat" title="Permalink to this heading">¶</a></h2>
<p>Quantization-Aware Training (QAT) refers to simulating quantization numerics during
training or fine-tuning, with the end goal of ultimately producing a higher quality
quantized model compared to simple post-training quantization (PTQ). During QAT,
the weights and/or activations are “fake quantized”, meaning they are transformed
as if they were being quantized, but kept in the original data type (e.g. bfloat16)
without being actually cast to lower bit-widths. Thus, fake quantization allows the
model to adjust for quantization noise when updating the weights, hence the training
process is “aware” that the model will ultimately be quantized after training.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># PTQ: x_q is quantized and cast to int8</span>
<span class="c1"># scale and zero point (zp) refer to parameters used to quantize x_float</span>
<span class="c1"># qmin and qmax refer to the range of quantized values</span>
<span class="n">x_q</span> <span class="o">=</span> <span class="p">(</span><span class="n">x_float</span> <span class="o">/</span> <span class="n">scale</span> <span class="o">+</span> <span class="n">zp</span><span class="p">)</span><span class="o">.</span><span class="n">round</span><span class="p">()</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">qmin</span><span class="p">,</span> <span class="n">qmax</span><span class="p">)</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">int8</span><span class="p">)</span>

<span class="c1"># QAT: x_fq is still in float</span>
<span class="c1"># Fake quantize simulates the numerics of quantize + dequantize</span>
<span class="n">x_fq</span> <span class="o">=</span> <span class="p">(</span><span class="n">x_float</span> <span class="o">/</span> <span class="n">scale</span> <span class="o">+</span> <span class="n">zp</span><span class="p">)</span><span class="o">.</span><span class="n">round</span><span class="p">()</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">qmin</span><span class="p">,</span> <span class="n">qmax</span><span class="p">)</span>
<span class="n">x_fq</span> <span class="o">=</span> <span class="p">(</span><span class="n">x_fq</span> <span class="o">-</span> <span class="n">zp</span><span class="p">)</span> <span class="o">*</span> <span class="n">scale</span>
</pre></div>
</div>
<p>QAT typically involves applying a transformation to your model before and after training.
For example, in the <a class="reference external" href="https://github.com/pytorch/ao/blob/v0.2.0/torchao/quantization/prototype/qat.py">torchao QAT implementation</a>,
these are represented as the prepare and convert steps: (1) prepare inserts fake quantize
operations into linear layers, and (2) convert transforms the fake quantize operations
to actual quantize and dequantize operations after training, thereby producing a quantized
model (dequantize operations are typically fused with linear after lowering).
Between these two steps, training can proceed exactly as before.</p>
<img alt="../_images/qat_diagram.png" src="../_images/qat_diagram.png" />
</section>
<section id="applying-qat-to-llama3-models">
<h2>Applying QAT to Llama3 models<a class="headerlink" href="#applying-qat-to-llama3-models" title="Permalink to this heading">¶</a></h2>
<p>We can easily apply the above QAT transformations to Llama3 in torchtune for fine-tuning:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torchtune.utils.quantization</span> <span class="kn">import</span> <span class="n">Int8DynActInt4WeightQATQuantizer</span>
<span class="kn">from</span> <span class="nn">torchtune.models.llama3</span> <span class="kn">import</span> <span class="n">llama3_8b</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">llama3_8b</span><span class="p">()</span>

<span class="c1"># Quantizer for int8 dynamic per token activations +</span>
<span class="c1"># int4 grouped per channel weights, only for linear layers</span>
<span class="n">quantizer</span> <span class="o">=</span> <span class="n">Int8DynActInt4WeightQATQuantizer</span><span class="p">()</span>

<span class="c1"># Insert &quot;fake quantize&quot; operations into linear layers.</span>
<span class="c1"># These operations simulate quantization numerics during</span>
<span class="c1"># fine-tuning without performing any dtype casting</span>
<span class="n">prepared_model</span> <span class="o">=</span> <span class="n">quantizer</span><span class="o">.</span><span class="n">prepare</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
<p>If we print the model we’ll see that all linear layers have been swapped with
<code class="code docutils literal notranslate"><span class="pre">Int8DynActInt4WeightQATLinear</span></code>, which simulates the numerics of int8
dynamic per token activations + int4 grouped per channel weights. Now the model
is ready for fine-tuning.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>&gt;&gt;&gt;<span class="w"> </span>print<span class="o">(</span>model.layers<span class="o">[</span><span class="m">0</span><span class="o">]</span>.attn<span class="o">)</span>
CausalSelfAttention<span class="o">(</span>
<span class="w">  </span><span class="o">(</span>q_proj<span class="o">)</span>:<span class="w"> </span>Linear<span class="o">(</span><span class="nv">in_features</span><span class="o">=</span><span class="m">4096</span>,<span class="w"> </span><span class="nv">out_features</span><span class="o">=</span><span class="m">4096</span>,<span class="w"> </span><span class="nv">bias</span><span class="o">=</span>False<span class="o">)</span>
<span class="w">  </span><span class="o">(</span>k_proj<span class="o">)</span>:<span class="w"> </span>Linear<span class="o">(</span><span class="nv">in_features</span><span class="o">=</span><span class="m">4096</span>,<span class="w"> </span><span class="nv">out_features</span><span class="o">=</span><span class="m">1024</span>,<span class="w"> </span><span class="nv">bias</span><span class="o">=</span>False<span class="o">)</span>
<span class="w">  </span><span class="o">(</span>v_proj<span class="o">)</span>:<span class="w"> </span>Linear<span class="o">(</span><span class="nv">in_features</span><span class="o">=</span><span class="m">4096</span>,<span class="w"> </span><span class="nv">out_features</span><span class="o">=</span><span class="m">1024</span>,<span class="w"> </span><span class="nv">bias</span><span class="o">=</span>False<span class="o">)</span>
<span class="w">  </span><span class="o">(</span>output_proj<span class="o">)</span>:<span class="w"> </span>Linear<span class="o">(</span><span class="nv">in_features</span><span class="o">=</span><span class="m">4096</span>,<span class="w"> </span><span class="nv">out_features</span><span class="o">=</span><span class="m">4096</span>,<span class="w"> </span><span class="nv">bias</span><span class="o">=</span>False<span class="o">)</span>
<span class="w">  </span><span class="o">(</span>pos_embeddings<span class="o">)</span>:<span class="w"> </span>RotaryPositionalEmbeddings<span class="o">()</span>
<span class="o">)</span>

&gt;&gt;&gt;<span class="w"> </span>print<span class="o">(</span>prepared_model.layers<span class="o">[</span><span class="m">0</span><span class="o">]</span>.attn<span class="o">)</span>
CausalSelfAttention<span class="o">(</span>
<span class="w">  </span><span class="o">(</span>q_proj<span class="o">)</span>:<span class="w"> </span>Int8DynActInt4WeightQATLinear<span class="o">(</span><span class="nv">in_features</span><span class="o">=</span><span class="m">4096</span>,<span class="w"> </span><span class="nv">out_features</span><span class="o">=</span><span class="m">4096</span>,<span class="w"> </span><span class="nv">bias</span><span class="o">=</span>False<span class="o">)</span>
<span class="w">  </span><span class="o">(</span>k_proj<span class="o">)</span>:<span class="w"> </span>Int8DynActInt4WeightQATLinear<span class="o">(</span><span class="nv">in_features</span><span class="o">=</span><span class="m">4096</span>,<span class="w"> </span><span class="nv">out_features</span><span class="o">=</span><span class="m">1024</span>,<span class="w"> </span><span class="nv">bias</span><span class="o">=</span>False<span class="o">)</span>
<span class="w">  </span><span class="o">(</span>v_proj<span class="o">)</span>:<span class="w"> </span>Int8DynActInt4WeightQATLinear<span class="o">(</span><span class="nv">in_features</span><span class="o">=</span><span class="m">4096</span>,<span class="w"> </span><span class="nv">out_features</span><span class="o">=</span><span class="m">1024</span>,<span class="w"> </span><span class="nv">bias</span><span class="o">=</span>False<span class="o">)</span>
<span class="w">  </span><span class="o">(</span>output_proj<span class="o">)</span>:<span class="w"> </span>Int8DynActInt4WeightQATLinear<span class="o">(</span><span class="nv">in_features</span><span class="o">=</span><span class="m">4096</span>,<span class="w"> </span><span class="nv">out_features</span><span class="o">=</span><span class="m">4096</span>,<span class="w"> </span><span class="nv">bias</span><span class="o">=</span>False<span class="o">)</span>
<span class="w">  </span><span class="o">(</span>pos_embeddings<span class="o">)</span>:<span class="w"> </span>RotaryPositionalEmbeddings<span class="o">()</span>
<span class="o">)</span>
</pre></div>
</div>
<p>After fine-tuning, we can convert the model to get an actual quantized model.
If we print the converted model, we’ll see that the QAT linears have been
swapped with <code class="code docutils literal notranslate"><span class="pre">Int8DynActInt4WeightLinear</span></code>, which are the quantized versions
of the linear layers. This quantized model can then be saved to checkpoint and
used for inference or generation.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Fine-tune as before</span>
<span class="n">train_loop</span><span class="p">(</span><span class="n">prepared_model</span><span class="p">)</span>

<span class="c1"># Convert fake quantize to actual quantize operations</span>
<span class="n">converted_model</span> <span class="o">=</span> <span class="n">quantizer</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="n">prepared_model</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>&gt;&gt;&gt;<span class="w"> </span>print<span class="o">(</span>converted_model.layers<span class="o">[</span><span class="m">0</span><span class="o">]</span>.attn<span class="o">)</span>
CausalSelfAttention<span class="o">(</span>
<span class="w">  </span><span class="o">(</span>q_proj<span class="o">)</span>:<span class="w"> </span>Int8DynActInt4WeightLinear<span class="o">()</span>
<span class="w">  </span><span class="o">(</span>k_proj<span class="o">)</span>:<span class="w"> </span>Int8DynActInt4WeightLinear<span class="o">()</span>
<span class="w">  </span><span class="o">(</span>v_proj<span class="o">)</span>:<span class="w"> </span>Int8DynActInt4WeightLinear<span class="o">()</span>
<span class="w">  </span><span class="o">(</span>output_proj<span class="o">)</span>:<span class="w"> </span>Int8DynActInt4WeightLinear<span class="o">()</span>
<span class="w">  </span><span class="o">(</span>pos_embeddings<span class="o">)</span>:<span class="w"> </span>RotaryPositionalEmbeddings<span class="o">()</span>
<span class="o">)</span>
</pre></div>
</div>
</section>
<section id="qat-finetuning-recipe-in-torchtune">
<h2>QAT finetuning recipe in torchtune<a class="headerlink" href="#qat-finetuning-recipe-in-torchtune" title="Permalink to this heading">¶</a></h2>
<p>Putting it all together, we can now fine-tune a model using torchtune’s <a class="reference external" href="https://github.com/pytorch/torchtune/blob/main/recipes/qat_distributed.py">QAT recipe</a>.
Make sure that you have first downloaded the Llama3 weights and tokenizer by
following <a class="reference internal" href="first_finetune_tutorial.html#download-llama-label"><span class="std std-ref">these instructions</span></a>. In this tutorial,
we use the following settings to demonstrate QAT’s effectiveness in recovering
quantization degradation compared to directly quantizing a model fine-tuned
without QAT. You can copy the default QAT config and make the following
modifications accordingly:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>tune<span class="w"> </span>cp<span class="w"> </span>llama3/8B_qat_full<span class="w"> </span>custom_8B_qat_full.yaml
</pre></div>
</div>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="c1"># Dataset</span>
<span class="nt">dataset</span><span class="p">:</span>
<span class="w">  </span><span class="nt">_component_</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">torchtune.datasets.text_completion_dataset</span>
<span class="w">  </span><span class="nt">source</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">allenai/c4</span>
<span class="w">  </span><span class="nt">max_seq_len</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">8192</span>
<span class="w">  </span><span class="nt">column</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">text</span>
<span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">en</span>
<span class="w">  </span><span class="nt">split</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">train</span>
<span class="nt">seed</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="nt">shuffle</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">True</span>

<span class="nn">...</span>

<span class="nt">epochs</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<span class="nt">max_steps_per_epoch</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">2000</span>
<span class="nt">fake_quant_after_n_steps</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1000</span>
<span class="nt">memory_efficient_fsdp_wrap</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">False</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>QAT in torchtune is currently not compatible with <code class="code docutils literal notranslate"><span class="pre">memory_efficient_fsdp_wrap</span></code>. This is a known issue and will be fixed in a future torchtune version.</p>
</div>
<p>Empirically, we observed that disabling fake quantization for the first N steps
led to better results, presumably because doing so allows the weights to stabilize
before we start introducing quantization noise to the fine-tuning process.
For this reason, here we disable fake quantization for the first 1000 steps.</p>
<p>You can then use the following command to run fine-tuning with QAT using the above
config. This workload requires at least 6 GPUs, each with VRAM of at least 80GB.
By default, this uses the int8 dynamic per token activations + int4 grouped per
channel weights quantization configuration as shown above:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>tune<span class="w"> </span>run<span class="w"> </span>--nnodes<span class="w"> </span><span class="m">1</span><span class="w"> </span>--nproc_per_node<span class="w"> </span><span class="m">6</span><span class="w"> </span>qat_distributed<span class="w"> </span>--config<span class="w"> </span>custom_8B_qat_full.yaml
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Make sure to point to the location of your Llama3 weights and tokenizer. This can be done
either by adding <code class="code docutils literal notranslate"><span class="pre">checkpointer.checkpoint_files=[my_model_checkpoint_path]</span> <span class="pre">tokenizer_checkpoint=my_tokenizer_checkpoint_path</span></code>
or by directly modifying the <code class="code docutils literal notranslate"><span class="pre">8B_qat_full.yaml</span></code> file. See our <a class="reference internal" href="../deep_dives/configs.html#config-tutorial-label"><span class="std std-ref">All About Configs</span></a>
for more details on how you can easily clone and modify torchtune configs.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>QAT introduces memory and computation overheads compared to regular fine-tuning,
since fake quantization fundamentally involves extra ops and requires cloning
the weights to avoid mutating them when computing the fake quantized values.
In general, we expect around 30% decrease in fine-tuning speed for models like
Llama3-8B. With activation checkpointing, the increase in memory footprint per
GPU is minimal (&lt; 5GB per GPU).</p>
</div>
</section>
<section id="quantizing-the-qat-model">
<h2>Quantizing the QAT model<a class="headerlink" href="#quantizing-the-qat-model" title="Permalink to this heading">¶</a></h2>
<p>Note that the QAT recipe above produces an unquantized bfloat16 model. The model
structure is exactly the same as the model produced with regular full fine-tuning
without QAT, just with different weights. To actually get a quantized model,
copy and make the following modifications to the quantization config:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>tune<span class="w"> </span>cp<span class="w"> </span>quantization<span class="w"> </span>custom_quantization.yaml
</pre></div>
</div>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="c1"># Model arguments</span>
<span class="nt">model</span><span class="p">:</span>
<span class="w">  </span><span class="nt">_component_</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">torchtune.models.llama3.llama3_8b</span>

<span class="nt">checkpointer</span><span class="p">:</span>
<span class="w">  </span><span class="nt">_component_</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">torchtune.utils.FullModelMetaCheckpointer</span>
<span class="w">  </span><span class="nt">checkpoint_dir</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">&lt;your QAT checkpoint dir&gt;</span>
<span class="w">  </span><span class="nt">checkpoint_files</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">meta_model_0.pt</span><span class="p p-Indicator">]</span>
<span class="w">  </span><span class="nt">recipe_checkpoint</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="w">  </span><span class="nt">output_dir</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">&lt;your QAT checkpoint dir&gt;</span>
<span class="w">  </span><span class="nt">model_type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">LLAMA3</span>

<span class="nn">...</span>

<span class="nt">quantizer</span><span class="p">:</span>
<span class="w">  </span><span class="nt">_component_</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">torchtune.utils.quantization.Int8DynActInt4WeightQATQuantizer</span>
<span class="w">  </span><span class="nt">groupsize</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">256</span>
</pre></div>
</div>
<p>The following command performs the convert step in the QAT flow, which actually
quantizes the float model to a model with quantized weights:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>tune<span class="w"> </span>run<span class="w"> </span>quantize<span class="w"> </span>--config<span class="w"> </span>custom_quantization.yaml
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Make sure to use the same QAT quantizer you used to fine-tune your model,
otherwise the numerics will be off and the quantized model will perform poorly.</p>
</div>
</section>
<section id="evaluating-the-quantized-model">
<h2>Evaluating the quantized model<a class="headerlink" href="#evaluating-the-quantized-model" title="Permalink to this heading">¶</a></h2>
<p>Now that we have a quantized model, we can run some evaluations on it and compare the
results against regular fine-tuning without QAT (i.e. post-training quantization).
To achieve this, we use <a class="reference external" href="https://github.com/EleutherAI/lm-evaluation-harness">EleutherAI’s evaluation harness</a>
integrated in torchtune. First, copy the evaluation config and make the following changes:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>tune<span class="w"> </span>cp<span class="w"> </span>eleuther_evaluation<span class="w"> </span>custom_eleuther_evaluation.yaml
</pre></div>
</div>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="c1"># Model arguments</span>
<span class="nt">model</span><span class="p">:</span>
<span class="w">  </span><span class="nt">_component_</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">torchtune.models.llama3.llama3_8b</span>

<span class="nt">checkpointer</span><span class="p">:</span>
<span class="w">  </span><span class="nt">_component_</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">torchtune.utils.FullModelTorchTuneCheckpointer</span>
<span class="w">  </span><span class="nt">checkpoint_dir</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">&lt;your quantized model checkpoint dir&gt;</span>
<span class="w">  </span><span class="nt">checkpoint_files</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">meta_model_0-8da4w.pt</span><span class="p p-Indicator">]</span>
<span class="w">  </span><span class="nt">recipe_checkpoint</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="w">  </span><span class="nt">output_dir</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">&lt;your quantized model checkpoint dir&gt;</span>
<span class="w">  </span><span class="nt">model_type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">LLAMA3</span>

<span class="nn">...</span>

<span class="c1"># EleutherAI specific eval args</span>
<span class="nt">tasks</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&quot;hellaswag&quot;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&quot;wikitext&quot;</span><span class="p p-Indicator">]</span>
<span class="nt">limit</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="nt">max_seq_length</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">8192</span>
<span class="nt">batch_size</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">8</span>

<span class="nt">quantizer</span><span class="p">:</span>
<span class="w">  </span><span class="nt">_component_</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">torchtune.utils.quantization.Int8DynActInt4WeightQuantizer</span>
<span class="w">  </span><span class="nt">groupsize</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">256</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Since we are passing in a quantized model, be sure to use the corresponding
post-training quantizer instead of the QAT quantizer. For example, if you
used the <code class="code docutils literal notranslate"><span class="pre">Int8DynActInt4WeightQATQuantizer</span></code> during fine-tuning, you
should specify <code class="code docutils literal notranslate"><span class="pre">Int8DynActInt4WeightQuantizer</span></code> in this step. See the
<a class="reference external" href="https://github.com/pytorch/torchtune/blob/main/recipes/quantize.py">quantization recipe</a>
for a full list of supported quantizers.</p>
</div>
<p>Now run the evaluation recipe:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>tune<span class="w"> </span>run<span class="w"> </span>eleuther_eval<span class="w"> </span>--config<span class="w"> </span>my_eleuther_evaluation.yaml
</pre></div>
</div>
<p>The results should look something like this:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># QAT quantized model evaluation results (int8 activations + int4 weights)</span>

<span class="p">|</span><span class="w">  </span>Tasks<span class="w">  </span><span class="p">|</span>Version<span class="p">|</span>Filter<span class="p">|</span>n-shot<span class="p">|</span><span class="w">    </span>Metric<span class="w">     </span><span class="p">|</span>Value<span class="w"> </span><span class="p">|</span><span class="w">   </span><span class="p">|</span>Stderr<span class="p">|</span>
<span class="p">|</span>---------<span class="p">|</span>------:<span class="p">|</span>------<span class="p">|</span>-----:<span class="p">|</span>---------------<span class="p">|</span>-----:<span class="p">|</span>---<span class="p">|</span>------<span class="p">|</span>
<span class="p">|</span>wikitext<span class="w"> </span><span class="p">|</span><span class="w">      </span><span class="m">2</span><span class="p">|</span>none<span class="w">  </span><span class="p">|</span><span class="w">     </span><span class="m">0</span><span class="p">|</span>word_perplexity<span class="p">|</span><span class="m">9</span>.9148<span class="p">|</span>±<span class="w">  </span><span class="p">|</span>N/A<span class="w">   </span><span class="p">|</span>
<span class="p">|</span><span class="w">         </span><span class="p">|</span><span class="w">       </span><span class="p">|</span>none<span class="w">  </span><span class="p">|</span><span class="w">     </span><span class="m">0</span><span class="p">|</span>byte_perplexity<span class="p">|</span><span class="m">1</span>.5357<span class="p">|</span>±<span class="w">  </span><span class="p">|</span>N/A<span class="w">   </span><span class="p">|</span>
<span class="p">|</span><span class="w">         </span><span class="p">|</span><span class="w">       </span><span class="p">|</span>none<span class="w">  </span><span class="p">|</span><span class="w">     </span><span class="m">0</span><span class="p">|</span>bits_per_byte<span class="w">  </span><span class="p">|</span><span class="m">0</span>.6189<span class="p">|</span>±<span class="w">  </span><span class="p">|</span>N/A<span class="w">   </span><span class="p">|</span>
<span class="p">|</span>hellaswag<span class="p">|</span><span class="w">      </span><span class="m">1</span><span class="p">|</span>none<span class="w">  </span><span class="p">|</span><span class="w">     </span><span class="m">0</span><span class="p">|</span>acc<span class="w">            </span><span class="p">|</span><span class="m">0</span>.5687<span class="p">|</span>±<span class="w">  </span><span class="p">|</span><span class="m">0</span>.0049<span class="p">|</span>
<span class="p">|</span><span class="w">         </span><span class="p">|</span><span class="w">       </span><span class="p">|</span>none<span class="w">  </span><span class="p">|</span><span class="w">     </span><span class="m">0</span><span class="p">|</span>acc_norm<span class="w">       </span><span class="p">|</span><span class="m">0</span>.7536<span class="p">|</span>±<span class="w">  </span><span class="p">|</span><span class="m">0</span>.0043<span class="p">|</span>
</pre></div>
</div>
<p>Comparing these results to the model fine-tuned without QAT, we can see that
QAT was able to recover a significant portion of the quantization degradations
from the original unquantized model compared to PTQ. For example, normalized
accuracy in the hellaswag task dropped by 2.20% with PTQ but only 0.74% with
QAT when compared to the original unquantized model. Similarly, word perplexity
in the wikitext task increased by 2.048 with PTQ but only 1.190 with QAT (lower
is better).</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># PTQ quantized model evaluation results (int8 activations + int4 weights)</span>

<span class="p">|</span><span class="w">  </span>Tasks<span class="w">  </span><span class="p">|</span>Version<span class="p">|</span>Filter<span class="p">|</span>n-shot<span class="p">|</span><span class="w">    </span>Metric<span class="w">     </span><span class="p">|</span><span class="w"> </span>Value<span class="w"> </span><span class="p">|</span><span class="w">   </span><span class="p">|</span>Stderr<span class="p">|</span>
<span class="p">|</span>---------<span class="p">|</span>------:<span class="p">|</span>------<span class="p">|</span>-----:<span class="p">|</span>---------------<span class="p">|</span>------:<span class="p">|</span>---<span class="p">|</span>------<span class="p">|</span>
<span class="p">|</span>wikitext<span class="w"> </span><span class="p">|</span><span class="w">      </span><span class="m">2</span><span class="p">|</span>none<span class="w">  </span><span class="p">|</span><span class="w">     </span><span class="m">0</span><span class="p">|</span>word_perplexity<span class="p">|</span><span class="m">10</span>.7735<span class="p">|</span>±<span class="w">  </span><span class="p">|</span>N/A<span class="w">   </span><span class="p">|</span>
<span class="p">|</span><span class="w">         </span><span class="p">|</span><span class="w">       </span><span class="p">|</span>none<span class="w">  </span><span class="p">|</span><span class="w">     </span><span class="m">0</span><span class="p">|</span>byte_perplexity<span class="p">|</span><span class="w"> </span><span class="m">1</span>.5598<span class="p">|</span>±<span class="w">  </span><span class="p">|</span>N/A<span class="w">   </span><span class="p">|</span>
<span class="p">|</span><span class="w">         </span><span class="p">|</span><span class="w">       </span><span class="p">|</span>none<span class="w">  </span><span class="p">|</span><span class="w">     </span><span class="m">0</span><span class="p">|</span>bits_per_byte<span class="w">  </span><span class="p">|</span><span class="w"> </span><span class="m">0</span>.6413<span class="p">|</span>±<span class="w">  </span><span class="p">|</span>N/A<span class="w">   </span><span class="p">|</span>
<span class="p">|</span>hellaswag<span class="p">|</span><span class="w">      </span><span class="m">1</span><span class="p">|</span>none<span class="w">  </span><span class="p">|</span><span class="w">     </span><span class="m">0</span><span class="p">|</span>acc<span class="w">            </span><span class="p">|</span><span class="w"> </span><span class="m">0</span>.5481<span class="p">|</span>±<span class="w">  </span><span class="p">|</span><span class="m">0</span>.0050<span class="p">|</span>
<span class="p">|</span><span class="w">         </span><span class="p">|</span><span class="w">       </span><span class="p">|</span>none<span class="w">  </span><span class="p">|</span><span class="w">     </span><span class="m">0</span><span class="p">|</span>acc_norm<span class="w">       </span><span class="p">|</span><span class="w"> </span><span class="m">0</span>.7390<span class="p">|</span>±<span class="w">  </span><span class="p">|</span><span class="m">0</span>.0044<span class="p">|</span>
</pre></div>
</div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Float model evaluation results (bfloat16)</span>

<span class="p">|</span><span class="w">  </span>Tasks<span class="w">  </span><span class="p">|</span>Version<span class="p">|</span>Filter<span class="p">|</span>n-shot<span class="p">|</span><span class="w">    </span>Metric<span class="w">     </span><span class="p">|</span>Value<span class="w"> </span><span class="p">|</span><span class="w">   </span><span class="p">|</span>Stderr<span class="p">|</span>
<span class="p">|</span>---------<span class="p">|</span>------:<span class="p">|</span>------<span class="p">|</span>-----:<span class="p">|</span>---------------<span class="p">|</span>-----:<span class="p">|</span>---<span class="p">|</span>------<span class="p">|</span>
<span class="p">|</span>wikitext<span class="w"> </span><span class="p">|</span><span class="w">      </span><span class="m">2</span><span class="p">|</span>none<span class="w">  </span><span class="p">|</span><span class="w">     </span><span class="m">0</span><span class="p">|</span>word_perplexity<span class="p">|</span><span class="m">8</span>.7251<span class="p">|</span>±<span class="w">  </span><span class="p">|</span>N/A<span class="w">   </span><span class="p">|</span>
<span class="p">|</span><span class="w">         </span><span class="p">|</span><span class="w">       </span><span class="p">|</span>none<span class="w">  </span><span class="p">|</span><span class="w">     </span><span class="m">0</span><span class="p">|</span>byte_perplexity<span class="p">|</span><span class="m">1</span>.4994<span class="p">|</span>±<span class="w">  </span><span class="p">|</span>N/A<span class="w">   </span><span class="p">|</span>
<span class="p">|</span><span class="w">         </span><span class="p">|</span><span class="w">       </span><span class="p">|</span>none<span class="w">  </span><span class="p">|</span><span class="w">     </span><span class="m">0</span><span class="p">|</span>bits_per_byte<span class="w">  </span><span class="p">|</span><span class="m">0</span>.5844<span class="p">|</span>±<span class="w">  </span><span class="p">|</span>N/A<span class="w">   </span><span class="p">|</span>
<span class="p">|</span>hellaswag<span class="p">|</span><span class="w">      </span><span class="m">1</span><span class="p">|</span>none<span class="w">  </span><span class="p">|</span><span class="w">     </span><span class="m">0</span><span class="p">|</span>acc<span class="w">            </span><span class="p">|</span><span class="m">0</span>.5740<span class="p">|</span>±<span class="w">  </span><span class="p">|</span><span class="m">0</span>.0049<span class="p">|</span>
<span class="p">|</span><span class="w">         </span><span class="p">|</span><span class="w">       </span><span class="p">|</span>none<span class="w">  </span><span class="p">|</span><span class="w">     </span><span class="m">0</span><span class="p">|</span>acc_norm<span class="w">       </span><span class="p">|</span><span class="m">0</span>.7610<span class="p">|</span>±<span class="w">  </span><span class="p">|</span><span class="m">0</span>.0043<span class="p">|</span>
</pre></div>
</div>
<p>Thus, the QAT flow produced a quantized model that outperforms the post-training
quantized model. Importantly, the quantized model structure is identical in both
flows, and so the model size, memory usage, and all other performance
characteristics are also the same.</p>
<p>Note that although the weights are quantized to int4, the quantized model size
for both the QAT and the PTQ flows are 8.187 GB, while the original float model
is 14.958 GB. This is because this quantizer uses int8 to represent the weights
as PyTorch does not have native int4 dtype support. A more efficient representation
is to pack the int4 weights, which will halve the quantized model size. This is
what the Int4WeightOnlyQuantizer does, and the corresponding QAT quantizer will
be added in the future.</p>
</section>
<section id="lowering-qat-model-to-device-optional">
<h2>Lowering QAT model to device (optional)<a class="headerlink" href="#lowering-qat-model-to-device-optional" title="Permalink to this heading">¶</a></h2>
<p>One important motivation for quantizing a model is to be able to run it in resource
constrained environments. You can further lower your QAT Llama3 model to edge devices
such as smartphones using <a class="reference external" href="https://github.com/pytorch/executorch/">executorch</a> by
following <a class="reference external" href="https://github.com/pytorch/executorch/tree/main/examples/models/llama2">these instructions</a>.
For example, the following command lowers the model to the XNNPACK backend:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>-m<span class="w"> </span>examples.models.llama2.export_llama<span class="w"> </span>--checkpoint<span class="w"> </span>&lt;your<span class="w"> </span>QAT<span class="w"> </span>checkpoint&gt;<span class="w"> </span>-p<span class="w"> </span>&lt;params.json&gt;<span class="w"> </span>-kv<span class="w"> </span>--use_sdpa_with_kv_cache<span class="w"> </span>-X<span class="w"> </span>-qmode<span class="w"> </span>8da4w<span class="w"> </span>--group_size<span class="w"> </span><span class="m">256</span><span class="w"> </span>-d<span class="w"> </span>fp32<span class="w"> </span>--metadata<span class="w"> </span><span class="s1">&#39;{&quot;get_bos_id&quot;:128000, &quot;get_eos_id&quot;:128001}&#39;</span><span class="w"> </span>--embedding-quantize<span class="w"> </span><span class="m">4</span>,32<span class="w"> </span>--output_name<span class="o">=</span><span class="s2">&quot;llama3_8da4w.pte&quot;</span>
</pre></div>
</div>
<p>This results in a much smaller quantized model of size 3.881 GB. When benchmarked on a OnePlus 12 smartphone, this model also achieved the same inference and generation speeds as the post-training quantized model. This is because the model structures are the same across the two flows:</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 33%" />
<col style="width: 33%" />
<col style="width: 33%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"></th>
<th class="head"><p>QAT</p></th>
<th class="head"><p>PTQ</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Quantized model size</p></td>
<td><p>3.881 GB</p></td>
<td><p>3.881 GB</p></td>
</tr>
<tr class="row-odd"><td><p>Inference speed</p></td>
<td><p>9.709 tok/s</p></td>
<td><p>9.815 tok/s</p></td>
</tr>
<tr class="row-even"><td><p>Generation speed</p></td>
<td><p>11.316 tok/s</p></td>
<td><p>11.364 tok/s</p></td>
</tr>
</tbody>
</table>
</section>
</section>


             </article>
             
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="e2e_flow.html" class="btn btn-neutral float-right" title="End-to-End Workflow with torchtune" accesskey="n" rel="next">Next <img src="../_static/images/chevron-right-orange.svg" class="next-page"></a>
      
      
        <a href="qlora_finetune.html" class="btn btn-neutral" title="Finetuning Llama2 with QLoRA" accesskey="p" rel="prev"><img src="../_static/images/chevron-right-orange.svg" class="previous-page"> Previous</a>
      
    </div>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2023-present, torchtune Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">Finetuning Llama3 with QAT</a><ul>
<li><a class="reference internal" href="#what-is-qat">What is QAT?</a></li>
<li><a class="reference internal" href="#applying-qat-to-llama3-models">Applying QAT to Llama3 models</a></li>
<li><a class="reference internal" href="#qat-finetuning-recipe-in-torchtune">QAT finetuning recipe in torchtune</a></li>
<li><a class="reference internal" href="#quantizing-the-qat-model">Quantizing the QAT model</a></li>
<li><a class="reference internal" href="#evaluating-the-quantized-model">Evaluating the quantized model</a></li>
<li><a class="reference internal" href="#lowering-qat-model-to-device-optional">Lowering QAT model to device (optional)</a></li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
         <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
         <script src="../_static/jquery.js"></script>
         <script src="../_static/underscore.js"></script>
         <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="../_static/doctools.js"></script>
         <script src="../_static/clipboard.min.js"></script>
         <script src="../_static/copybutton.js"></script>
         <script src="../_static/design-tabs.js"></script>
     

  

  <script type="text/javascript" src="../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
<!-- Disabling "auto-collapsing" of sections on the left side bar. Replace script with commented out sections to reenable. -->
<!--  
<script script type="text/javascript">
    var collapsedSections = ['Introduction', 'Getting Started', 'Tutorials']
</script> -->

<script script type="text/javascript">
    var collapsedSections = []
</script>


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">Stay up to date</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          </ul>  
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch Podcasts</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
          </ul>
         </div>
        </div>
        
        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p>© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
          <a href="https://www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see <a href="https://www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
           <li class="resources-mobile-menu-title">
             <a>Learn</a>
           </li>
           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/get-started">Get Started</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials">Tutorials</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/beginner/basics/intro.html">Learn the Basics</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/recipes/recipes_index.html">PyTorch Recipes</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/beginner/introyt.html">Introduction to PyTorch - YouTube Series</a>
             </li>
           </ul>
           <li class="resources-mobile-menu-title">
             <a>Ecosystem</a>
           </li>
           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/ecosystem">Tools</a>
             </li>
             <li>
               <a href="https://pytorch.org/#community-module">Community</a>
             </li>
             <li>
               <a href="https://discuss.pytorch.org/">Forums</a>
             </li>
             <li>
               <a href="https://pytorch.org/resources">Developer Resources</a>
             </li>
             <li>
               <a href="https://pytorch.org/ecosystem/contributor-awards-2023">Contributor Awards - 2023</a>
             </li>
           </ul>

           <li class="resources-mobile-menu-title">
             <a>Edge</a>
           </li>

           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/edge">About PyTorch Edge</a>
             </li>
             
             <li>
               <a href="https://pytorch.org/executorch-overview">ExecuTorch</a>
             </li>
           </ul>

           <li class="resources-mobile-menu-title">
             <a>Docs</a>
           </li>

           <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/pytorch-domains">PyTorch Domains</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            <a>Blog & News</a>
          </li>
            
           <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/blog/">PyTorch Blog</a>
            </li>
            <li>
              <a href="https://pytorch.org/community-blog">Community Blog</a>
            </li>

            <li>
              <a href="https://pytorch.org/videos">Videos</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">Community Stories</a>
            </li>
            <li>
              <a href="https://pytorch.org/events">Events</a>
            </li>
          </ul>
          
          <li class="resources-mobile-menu-title">
            <a>About</a>
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
            </li>
            <li>
              <a href="https://pytorch.org/governing-board">Governing Board</a>
            </li>
          </ul>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>