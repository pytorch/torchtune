


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta name="robots" content="noindex">
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Finetuning Llama2 with QLoRA &mdash; torchtune main documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="../_static/sg_gallery.css" type="text/css" />
  <link rel="stylesheet" href="../_static/sg_gallery-binder.css" type="text/css" />
  <link rel="stylesheet" href="../_static/sg_gallery-dataframe.css" type="text/css" />
  <link rel="stylesheet" href="../_static/sg_gallery-rendered-html.css" type="text/css" />
  <link rel="stylesheet" href="../_static/tabs.css" type="text/css" />
  <link rel="stylesheet" href="../_static/sphinx-design.5ea377869091fd0449014c60fc090103.min.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/custom_torchtune.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Finetuning Llama3 with QAT" href="qat_finetune.html" />
    <link rel="prev" title="Finetuning Llama2 with LoRA" href="lora_finetune.html" />
  <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-T8XT4PS');</script>
    <!-- End Google Tag Manager -->
  

  
  <script src="../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Learn
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/get-started">
                  <span class=dropdown-title>Get Started</span>
                  <p>Run PyTorch locally or get started quickly with one of the supported cloud platforms</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials">
                  <span class="dropdown-title">Tutorials</span>
                  <p>Whats new in PyTorch tutorials</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/basics/intro.html">
                  <span class="dropdown-title">Learn the Basics</span>
                  <p>Familiarize yourself with PyTorch concepts and modules</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/recipes/recipes_index.html">
                  <span class="dropdown-title">PyTorch Recipes</span>
                  <p>Bite-size, ready-to-deploy PyTorch code examples</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/introyt.html">
                  <span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
                  <p>Master PyTorch basics with our engaging YouTube tutorial series</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Ecosystem
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem">
                  <span class="dropdown-title">Tools</span>
                  <p>Learn about the tools and frameworks in the PyTorch Ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class=dropdown-title>Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class=dropdown-title>Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class=dropdown-title>Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem/contributor-awards-2023">
                  <span class="dropdown-title">Contributor Awards - 2023</span>
                  <p>Award winners announced at this year's PyTorch Conference</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Edge
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/edge">
                  <span class="dropdown-title">About PyTorch Edge</span>
                  <p>Build innovative and privacy-aware AI experiences for edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/executorch-overview">
                  <span class="dropdown-title">ExecuTorch</span>
                  <p>End-to-end solution for enabling on-device inference capabilities across mobile and edge devices</p>
                </a>
              </div>
            </div>  
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p>Explore the documentation for comprehensive guidance on how to use PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/pytorch-domains">
                  <span class="dropdown-title">PyTorch Domains</span>
                  <p>Read the PyTorch Domains documentation to learn more about domain-specific libraries</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Blogs & News 
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/blog/">
                  <span class="dropdown-title">PyTorch Blog</span>
                  <p>Catch up on the latest technical news and happenings</p>
                </a>
                 <a class="nav-dropdown-item" href="https://pytorch.org/community-blog">
                  <span class="dropdown-title">Community Blog</span>
                  <p>Stories from the PyTorch ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/videos">
                  <span class="dropdown-title">Videos</span>
                  <p>Learn about the latest PyTorch tutorials, new, and more </p>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                About
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn more about the PyTorch Foundation</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/governing-board">
                  <span class="dropdown-title">Governing Board</span>
                  <p></p>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div class="no-dropdown">
              <a href="https://pytorch.org/join" data-cta="join">
                Become a Member
              </a>
            </div>
          </li>
          <li>
           <div class="main-menu-item">
             <a href="https://github.com/pytorch/pytorch" class="github-icon">
             </a>
           </div>
          </li>
          <!--- TODO: This block adds the search icon to the nav bar. We will enable it later. 
          <li>
            <div class="main-menu-item">
             <a href="https://github.com/pytorch/pytorch" class="search-icon">
             </a>
            </div>
          </li>
          --->
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
        <a href='https://pytorch.org/torchtune/versions.html'>main &#x25BC</a>
      </div>
    


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          </div>

          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../overview.html">torchtune Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install.html">Install Instructions</a></li>
<li class="toctree-l1"><a class="reference internal" href="first_finetune_tutorial.html">Fine-Tune Your First LLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tune_cli.html">torchtune CLI</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Tutorials</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="llama3.html">Meta Llama3 in torchtune</a></li>
<li class="toctree-l1"><a class="reference internal" href="lora_finetune.html">Finetuning Llama2 with LoRA</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Finetuning Llama2 with QLoRA</a></li>
<li class="toctree-l1"><a class="reference internal" href="qat_finetune.html">Finetuning Llama3 with QAT</a></li>
<li class="toctree-l1"><a class="reference internal" href="e2e_flow.html">End-to-End Workflow with torchtune</a></li>
<li class="toctree-l1"><a class="reference internal" href="datasets.html">Configuring Datasets for Fine-Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="chat.html">Fine-tuning Llama3 with Chat Data</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Deep-Dives</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../deep_dives/checkpointer.html">Checkpointing in torchtune</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deep_dives/configs.html">All About Configs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deep_dives/recipe_deepdive.html">What Are Recipes?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deep_dives/wandb_logging.html">Logging to Weights &amp; Biases</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../api_ref_config.html">torchtune.config</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_ref_data.html">torchtune.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_ref_datasets.html">torchtune.datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_ref_models.html">torchtune.models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_ref_modules.html">torchtune.modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_ref_utilities.html">torchtune.utils</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
      <li>Finetuning Llama2 with QLoRA</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="../_sources/tutorials/qlora_finetune.rst.txt" rel="nofollow"><img src="../_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          <!-- Google Tag Manager (noscript) -->
          <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS"
          height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
          <!-- End Google Tag Manager (noscript) -->
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <section id="finetuning-llama2-with-qlora">
<span id="qlora-finetune-label"></span><h1>Finetuning Llama2 with QLoRA<a class="headerlink" href="#finetuning-llama2-with-qlora" title="Permalink to this heading">¶</a></h1>
<p>In this tutorial, we’ll learn about <a class="reference external" href="https://arxiv.org/abs/2305.14314">QLoRA</a>, an enhancement on top of
<a class="reference external" href="https://arxiv.org/abs/2106.09685">LoRA</a> that maintains frozen model parameters in 4-bit quantized precision, thereby reducing memory usage. We’ll
walk through how QLoRA can be utilized within torchtune to finetune a Llama2-7b model in &lt;10 GB of memory.
It is highly recommended to first develop an understanding of <a class="reference internal" href="lora_finetune.html#lora-finetune-label"><span class="std std-ref">LoRA finetuning in torchtune</span></a>.</p>
<div class="sd-container-fluid sd-sphinx-override sd-mb-4 docutils">
<div class="sd-row sd-row-cols-2 sd-row-cols-xs-2 sd-row-cols-sm-2 sd-row-cols-md-2 sd-row-cols-lg-2 docutils">
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm docutils">
<div class="sd-card-body docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
<svg version="1.1" width="1.0em" height="1.0em" class="sd-octicon sd-octicon-mortar-board" viewBox="0 0 16 16" aria-hidden="true"><path d="M7.693 1.066a.747.747 0 0 1 .614 0l7.25 3.25a.75.75 0 0 1 0 1.368L13 6.831v2.794c0 1.024-.81 1.749-1.66 2.173-.893.447-2.075.702-3.34.702-.278 0-.55-.012-.816-.036a.75.75 0 0 1 .133-1.494c.22.02.45.03.683.03 1.082 0 2.025-.221 2.67-.543.69-.345.83-.682.83-.832V7.503L8.307 8.934a.747.747 0 0 1-.614 0L4 7.28v1.663c.296.105.575.275.812.512.438.438.688 1.059.688 1.796v3a.75.75 0 0 1-.75.75h-3a.75.75 0 0 1-.75-.75v-3c0-.737.25-1.358.688-1.796.237-.237.516-.407.812-.512V6.606L.443 5.684a.75.75 0 0 1 0-1.368ZM2.583 5 8 7.428 13.416 5 8 2.572ZM2.5 11.25v2.25H4v-2.25c0-.388-.125-.611-.25-.735a.697.697 0 0 0-.5-.203.707.707 0 0 0-.5.203c-.125.124-.25.347-.25.735Z"></path></svg> What you will learn</div>
<ul class="simple">
<li><p class="sd-card-text">How QLoRA saves memory over LoRA finetuning</p></li>
<li><p class="sd-card-text">An overview of QLoRA in torchtune</p></li>
<li><p class="sd-card-text">How to run a QLoRA finetune in torchtune</p></li>
</ul>
</div>
</div>
</div>
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm docutils">
<div class="sd-card-body docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
<svg version="1.1" width="1.0em" height="1.0em" class="sd-octicon sd-octicon-list-unordered" viewBox="0 0 16 16" aria-hidden="true"><path d="M5.75 2.5h8.5a.75.75 0 0 1 0 1.5h-8.5a.75.75 0 0 1 0-1.5Zm0 5h8.5a.75.75 0 0 1 0 1.5h-8.5a.75.75 0 0 1 0-1.5Zm0 5h8.5a.75.75 0 0 1 0 1.5h-8.5a.75.75 0 0 1 0-1.5ZM2 14a1 1 0 1 1 0-2 1 1 0 0 1 0 2Zm1-6a1 1 0 1 1-2 0 1 1 0 0 1 2 0ZM2 4a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z"></path></svg> Prerequisites</div>
<ul class="simple">
<li><p class="sd-card-text">Be familiar with <a class="reference internal" href="../overview.html#overview-label"><span class="std std-ref">torchtune</span></a></p></li>
<li><p class="sd-card-text">Make sure to <a class="reference internal" href="../install.html#install-label"><span class="std std-ref">install torchtune</span></a></p></li>
<li><p class="sd-card-text">Make sure you have downloaded the <a class="reference internal" href="first_finetune_tutorial.html#download-llama-label"><span class="std std-ref">Llama2-7B model weights</span></a></p></li>
<li><p class="sd-card-text">Be familiar with <a class="reference internal" href="lora_finetune.html#lora-finetune-label"><span class="std std-ref">LoRA in torchtune</span></a></p></li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section id="what-is-qlora">
<h2>What is QLoRA?<a class="headerlink" href="#what-is-qlora" title="Permalink to this heading">¶</a></h2>
<p>QLoRA builds on top of LoRA to enable further
memory savings. In LoRA, model parameters can be thought of as existing in two partitions: adapters, which are
low-rank matrices added to different layers of a neural network, and base model parameters, which are parameters that are part of
the original model. In vanilla LoRA-style training, both these parameters are held in the same precision (typically fp32 or <a class="reference external" href="https://en.wikipedia.org/wiki/Bfloat16_floating-point_format#bfloat16_floating-point_format">bf16</a>.), and
therefore activations and intermediate gradients computed are in fp32/bf16.</p>
<p>QLoRA further quantizes the base model parameters into a bespoke 4-bit NormalFloat (<a class="reference external" href="https://www.youtube.com/watch?v=TPcXVJ1VSRI&amp;t=563s">NF4</a>) data type, resulting in 4-8x less parameter memory usage while
largely retaining model accuracy. As a result, the vast majority of parameters only take up 4 bits (as opposed to 16 or 32 bits by bf16/fp32 dtypes). This
quantization is done through the method highlighted in the original <a class="reference external" href="https://arxiv.org/abs/2305.14314">QLoRA paper</a>. Adapter
parameters are still held in the original precision, and activations, gradients, and optimizer states still exist in the higher precision to preserve
accuracy.</p>
<p>The QLoRA authors introduce two key abstractions to decrease memory usage and avoid accuracy degradation: the bespoke 4-bit NormatFloat
type, and a double quantization method that quantizes the quantization parameters themselves to save even more memory. torchtune uses
the <a class="reference external" href="https://github.com/pytorch-labs/ao/blob/b9beaf351e27133d189b57d6fa725b1a7824a457/torchao/dtypes/nf4tensor.py#L153">NF4Tensor</a> abstraction from the <a class="reference external" href="https://github.com/pytorch-labs/ao">torchao library</a> to build QLoRA components as specified in the paper.
torchao is a PyTorch-native library that allows you to quantize and prune your models.</p>
</section>
<section id="using-qlora-to-save-memory">
<span id="qlora-core-highlevel"></span><h2>Using QLoRA to save memory<a class="headerlink" href="#using-qlora-to-save-memory" title="Permalink to this heading">¶</a></h2>
<p>In this section, we’ll overview how to apply QLoRA to a <a class="reference internal" href="../generated/torchtune.modules.peft.LoRALinear.html#torchtune.modules.peft.LoRALinear" title="torchtune.modules.peft.LoRALinear"><code class="xref py py-class docutils literal notranslate"><span class="pre">LoRALinear</span></code></a> layer in torchtune. For a deep dive into details on QLoRA in torchtune and underlying abstractions,
please see the <a class="reference internal" href="#qlora-deepdive-label"><span class="std std-ref">QLoRA in torchtune deepdive</span></a> section of this tutorial.</p>
<p>A core idea of QLoRA is the distinction between compute and storage datatypes (dtypes). Specifically, QLoRA stores base model parameters in 4-bit precision (i.e. the storage dtype), and runs
computation in an original higher precision (the compute dtype), generally either fp32 or bf16. As a first step, QLoRA needs to quantize these base model parameters to 4-bit precision
and store them.</p>
<p>To quantize a <a class="reference internal" href="../generated/torchtune.modules.peft.LoRALinear.html#torchtune.modules.peft.LoRALinear" title="torchtune.modules.peft.LoRALinear"><code class="xref py py-class docutils literal notranslate"><span class="pre">LoRALinear</span></code></a> layer in the QLoRA style, simply pass in the <code class="docutils literal notranslate"><span class="pre">quantize_base</span></code> flag as <code class="docutils literal notranslate"><span class="pre">True</span></code> into <a class="reference internal" href="../generated/torchtune.modules.peft.LoRALinear.html#torchtune.modules.peft.LoRALinear" title="torchtune.modules.peft.LoRALinear"><code class="xref py py-class docutils literal notranslate"><span class="pre">LoRALinear</span></code></a>. This flag
will result in base model weights being quantized and backed by the <code class="docutils literal notranslate"><span class="pre">NF4Tensor</span></code> dtype. Forward passes will also be automatically handled to work with the <code class="docutils literal notranslate"><span class="pre">NF4Tensor</span></code> dtype,
specifically, the <code class="docutils literal notranslate"><span class="pre">NF4</span></code> base weight will be de-quantized to the compute precision, activation will be computed, and only the 4-bit parameter will be stored for gradient computation
in the backward pass, avoiding extra memory usage that would be incurred by storing the higher precision compute dtype.</p>
<p>Here’s an example of creating a quantized <code class="docutils literal notranslate"><span class="pre">LoRALinear</span></code> layer in comparison to an unquantized <code class="docutils literal notranslate"><span class="pre">LoRALinear</span></code> layer. As we can see, the quantized layer consumes
~8x less memory than the unquantized counterpart.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torchtune.modules.peft</span> <span class="kn">import</span> <span class="n">LoRALinear</span>

<span class="n">torch</span><span class="o">.</span><span class="n">set_default_device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
<span class="n">qlora_linear</span> <span class="o">=</span> <span class="n">LoRALinear</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">quantize_base</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">memory_allocated</span><span class="p">())</span>  <span class="c1"># 177,152 bytes</span>
<span class="k">del</span> <span class="n">qlora_linear</span>
<span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
<span class="n">lora_linear</span> <span class="o">=</span> <span class="n">LoRALinear</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">quantize_base</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">memory_allocated</span><span class="p">())</span> <span class="c1"># 1,081,344 bytes</span>
</pre></div>
</div>
</section>
<section id="using-qlora-in-torchtune">
<h2>Using QLoRA in torchtune<a class="headerlink" href="#using-qlora-in-torchtune" title="Permalink to this heading">¶</a></h2>
<p>We’ll now cover how you can initialize a QLoRA-enabled Llama2-7b model as well as some details around
checkpointing with QLoRA.</p>
<p>With torchtune, you can use a simple builder similar to the LoRA builder (<a class="reference internal" href="../generated/torchtune.models.llama2.lora_llama2_7b.html#torchtune.models.llama2.lora_llama2_7b" title="torchtune.models.llama2.lora_llama2_7b"><code class="xref py py-func docutils literal notranslate"><span class="pre">lora_llama_2_7b</span></code></a>) to apply QLoRA to Llama2 models. Here’s a simple example of
initializing a Llama2-7b model with QLoRA enabled:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torchtune.models.llama2</span> <span class="kn">import</span> <span class="n">qlora_llama2_7b</span>

<span class="n">qlora_model</span> <span class="o">=</span> <span class="n">qlora_llama2_7b</span><span class="p">(</span><span class="n">lora_attn_modules</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;q_proj&quot;</span><span class="p">,</span> <span class="s2">&quot;v_proj&quot;</span><span class="p">])</span>
</pre></div>
</div>
<p>Under the hood, this will apply LoRA to the <code class="docutils literal notranslate"><span class="pre">q_proj</span></code> and <code class="docutils literal notranslate"><span class="pre">v_proj</span></code> matrices in all attention layers, and further quantize the base parameters
in these matrices to the <code class="docutils literal notranslate"><span class="pre">NF4</span></code> dtype. Note that quantization of base model parameters is only applied to layers that are configured to have
LoRA adapters added. For example, in this case, <code class="docutils literal notranslate"><span class="pre">k_proj</span></code> and <code class="docutils literal notranslate"><span class="pre">output_proj</span></code> in the attention layers don’t have LoRA applied to them, so their
base model parameters are not quantized. We can see this by printing the base model parameter dtypes for a particular attention layer:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">attn</span> <span class="o">=</span> <span class="n">qlora_model</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">attn</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">attn</span><span class="o">.</span><span class="n">q_proj</span><span class="o">.</span><span class="n">weight</span><span class="p">))</span>  <span class="c1"># &lt;class &#39;torchao.dtypes.nf4tensor.NF4Tensor&#39;&gt;</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">attn</span><span class="o">.</span><span class="n">k_proj</span><span class="o">.</span><span class="n">weight</span><span class="p">))</span>  <span class="c1"># &lt;class &#39;torch.nn.parameter.Parameter&#39;&gt;</span>
</pre></div>
</div>
<p>Next, there are a couple of details essential to checkpointing (i.e. <code class="docutils literal notranslate"><span class="pre">state_dict</span></code>) of QLoRA-enabled models.
To integrate well with torchtune’s <a class="reference internal" href="../api_ref_utilities.html#checkpointing-label"><span class="std std-ref">checkpointing</span></a>, we need to convert <code class="docutils literal notranslate"><span class="pre">NF4Tensors</span></code> back to their
original precision (generally fp32/bf16). This allows QLoRA-trained checkpoints to interoperate well with the rest of the ecosystem, within
torchtune and beyond (e.g. post-training quantization, evaluation, inference). This conversion process also allows LoRA adapter weights to be merged back into the base model as done
in a typical LoRA training flow.</p>
<p>To achieve this, when using torchtune’s <a class="reference internal" href="../generated/torchtune.models.llama2.lora_llama2_7b.html#torchtune.models.llama2.lora_llama2_7b" title="torchtune.models.llama2.lora_llama2_7b"><code class="xref py py-func docutils literal notranslate"><span class="pre">lora_llama_2_7b</span></code></a> builder, we automatically register a hook,
<a class="reference internal" href="../generated/torchtune.modules.common_utils.reparametrize_as_dtype_state_dict_post_hook.html#torchtune.modules.common_utils.reparametrize_as_dtype_state_dict_post_hook" title="torchtune.modules.common_utils.reparametrize_as_dtype_state_dict_post_hook"><code class="xref py py-func docutils literal notranslate"><span class="pre">reparametrize_as_dtype_state_dict_post_hook</span></code></a>,
that runs after calling <code class="docutils literal notranslate"><span class="pre">.state_dict()</span></code> on the top level model. This hook converts <code class="docutils literal notranslate"><span class="pre">NF4Tensors</span></code> back to their original precision, while also offloading these
converted tensors to the CPU. This offloading is to avoid peaking memory; if we did not, we would have to maintain an entire bf16/fp32 copy of the <code class="docutils literal notranslate"><span class="pre">state_dict</span></code>
on GPU.</p>
</section>
<section id="putting-it-all-together-qlora-finetune">
<h2>Putting it all together: QLoRA finetune<a class="headerlink" href="#putting-it-all-together-qlora-finetune" title="Permalink to this heading">¶</a></h2>
<p>Putting it all together, we can now finetune a model using torchtune’s <a class="reference external" href="https://github.com/pytorch/torchtune/blob/48626d19d2108f92c749411fbd5f0ff140023a25/recipes/lora_finetune.py">LoRA recipe</a>,
with a <a class="reference external" href="https://github.com/pytorch/torchtune/blob/main/recipes/configs/llama2/7B_qlora_single_device.yaml">QLoRA configuration</a>.</p>
<p>Make sure that you have first downloaded the Llama2 weights and tokenizer by following <a class="reference internal" href="first_finetune_tutorial.html#download-llama-label"><span class="std std-ref">these instructions</span></a>.
You can then run the following command to perform a QLoRA finetune of Llama2-7B on a single GPU.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>tune<span class="w"> </span>run<span class="w"> </span>lora_finetune_single_device<span class="w"> </span>--config<span class="w"> </span>llama2/7B_qlora_single_device
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Make sure to correctly point to the location of your Llama2 weights and tokenizer. This can be done
either by adding <code class="code docutils literal notranslate"><span class="pre">checkpointer.checkpoint_files=[my_model_checkpoint_path]</span> <span class="pre">tokenizer_checkpoint=my_tokenizer_checkpoint_path</span></code>
or by directly modifying the <code class="code docutils literal notranslate"><span class="pre">7B_qlora_single_device.yaml</span></code> file. See our “<a class="reference internal" href="../deep_dives/configs.html#config-tutorial-label"><span class="std std-ref">All About Configs</span></a>” recipe
for more details on how you can easily clone and modify torchtune configs.</p>
</div>
<p>By default, this run should log peak memory stats at model initialization time and every 100
iterations during training. Let’s understand the memory savings enabled by QLoRA on top of LoRA training. LoRA training
can be run as follows:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>tune<span class="w"> </span>run<span class="w"> </span>lora_finetune_single_device<span class="w"> </span>--config<span class="w"> </span>llama2/7B_lora_single_device
</pre></div>
</div>
<p>You should see the memory usage printed out during model initialization and training. An example log for LoRA model initialization is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Memory</span> <span class="n">Stats</span> <span class="n">after</span> <span class="n">model</span> <span class="n">init</span><span class="p">::</span>
<span class="n">GPU</span> <span class="n">peak</span> <span class="n">memory</span> <span class="n">allocation</span><span class="p">:</span> <span class="mf">13.96</span> <span class="n">GB</span>
<span class="n">GPU</span> <span class="n">peak</span> <span class="n">memory</span> <span class="n">reserved</span><span class="p">:</span> <span class="mf">13.98</span> <span class="n">GB</span>
<span class="n">GPU</span> <span class="n">peak</span> <span class="n">memory</span> <span class="n">active</span><span class="p">:</span> <span class="mf">13.96</span> <span class="n">GB</span>
</pre></div>
</div>
<p>The following table compares the QLoRA’s memory reserved during model initialization and training against vanilla LoRA’s.
We can see that QLoRA reduces peak memory by about 35% during model initialization, and about 40% during model training:</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Finetuning method</p></th>
<th class="head"><p>Peak memory reserved, model init</p></th>
<th class="head"><p>Peak memory reserved, training</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>LoRA</p></td>
<td><p>13.98 GB</p></td>
<td><p>15.57 GB</p></td>
</tr>
<tr class="row-odd"><td><p>QLoRA</p></td>
<td><p>9.13 GB</p></td>
<td><p>9.29 GB</p></td>
</tr>
</tbody>
</table>
<p>From the logs, one can see that the out-of-the-box training performance is quite slow, slower than 1 iteration per
second:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="mi">1</span><span class="o">|</span><span class="mi">149</span><span class="o">|</span><span class="n">Loss</span><span class="p">:</span> <span class="mf">0.9157477021217346</span><span class="p">:</span>   <span class="mi">1</span><span class="o">%|</span>          <span class="o">|</span> <span class="mi">149</span><span class="o">/</span><span class="mi">25880</span> <span class="p">[</span><span class="mi">02</span><span class="p">:</span><span class="mi">08</span><span class="o">&lt;</span><span class="mi">6</span><span class="p">:</span><span class="mi">14</span><span class="p">:</span><span class="mi">19</span><span class="p">,</span>  <span class="mf">1.15</span><span class="n">it</span><span class="o">/</span><span class="n">s</span>
</pre></div>
</div>
<p>To speed things up, we can leverage <a class="reference external" href="https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html">torch.compile</a> to compile our model and run the compiled result. To work with
QLoRA training, a nightly build of PyTorch must be used. To update PyTorch to the latest nightly,
please see <a class="reference external" href="https://pytorch.org/get-started/locally/">the installation instructions</a>. Once updated,
you can specify the compile flag as <code class="docutils literal notranslate"><span class="pre">True</span></code> via a config override:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>tune<span class="w"> </span>run<span class="w"> </span>lora_finetune_single_device<span class="w"> </span>--config<span class="w"> </span>llama2/7B_qlora_single_device<span class="w"> </span><span class="nv">compile</span><span class="o">=</span>True
</pre></div>
</div>
<p>From the logs, we can see about a 200% speed up (after a few hundred iterations once the training has stabilized):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="mi">1</span><span class="o">|</span><span class="mi">228</span><span class="o">|</span><span class="n">Loss</span><span class="p">:</span> <span class="mf">0.8158286809921265</span><span class="p">:</span>   <span class="mi">1</span><span class="o">%|</span>          <span class="o">|</span> <span class="mi">228</span><span class="o">/</span><span class="mi">25880</span> <span class="p">[</span><span class="mi">11</span><span class="p">:</span><span class="mi">59</span><span class="o">&lt;</span><span class="mi">1</span><span class="p">:</span><span class="mi">48</span><span class="p">:</span><span class="mi">16</span><span class="p">,</span>  <span class="mf">3.95</span><span class="n">it</span><span class="o">/</span><span class="n">s</span>
</pre></div>
</div>
<p>A comparison of the smoothed loss curves between QLoRA and LoRA can be seen below.</p>
<img alt="../_images/qlora_exp.png" src="../_images/qlora_exp.png" />
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The above figure was generated with W&amp;B. You can use torchtune’s <a class="reference internal" href="../generated/torchtune.utils.metric_logging.WandBLogger.html#torchtune.utils.metric_logging.WandBLogger" title="torchtune.utils.metric_logging.WandBLogger"><code class="xref py py-class docutils literal notranslate"><span class="pre">WandBLogger</span></code></a>
to generate similar loss curves, but you will need to install W&amp;B and setup an account separately. For more details on
using W&amp;B in torchtune, see our “<a class="reference internal" href="../deep_dives/wandb_logging.html#wandb-logging"><span class="std std-ref">Logging to Weights &amp; Biases</span></a>” recipe.</p>
</div>
<p>As an exercise, you can also try running some evaluation tasks or manually inspecting generations
output by your saved checkpoints (which can be found in <code class="code docutils literal notranslate"><span class="pre">output_dir</span></code>).</p>
<p>In the final section, we’ll go over a deep dive on how a QLoRA component can be built from a LoRA component.</p>
</section>
<section id="deep-dive-building-qlora-from-lora">
<span id="qlora-deepdive-label"></span><h2>Deep-dive: Building QLoRA from LoRA<a class="headerlink" href="#deep-dive-building-qlora-from-lora" title="Permalink to this heading">¶</a></h2>
<p>This deep-dive section resumes from the <a class="reference internal" href="#qlora-core-highlevel"><span class="std std-ref">Using QLoRA to save memory</span></a> portion of this tutorial and dives into how quantization is done with <code class="docutils literal notranslate"><span class="pre">NF4Tensor</span></code> and handled appropriately in the forward pass.</p>
<p>First, we’ll begin with
a vanilla minimal LoRA layer, taken from <a class="reference internal" href="lora_finetune.html#lora-finetune-label"><span class="std std-ref">the LoRA tutorial</span></a> and augmented to support quantization:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span><span class="p">,</span> <span class="n">Tensor</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="hll"><span class="kn">from</span> <span class="nn">torchao.dtypes.nf4tensor</span> <span class="kn">import</span> <span class="n">linear_nf4</span><span class="p">,</span> <span class="n">to_nf4</span>
</span>
<span class="k">class</span> <span class="nc">LoRALinear</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">in_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">out_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">rank</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">alpha</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
    <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
<span class="hll">    <span class="n">quantize_base</span><span class="p">:</span> <span class="nb">bool</span>
</span>  <span class="p">):</span>
    <span class="c1"># These are the weights from the original pretrained model</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">linear_weight</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">weight</span>
    <span class="c1"># Use torchao&#39;s to_nf4 API to quantize the base weight if needed.</span>
<span class="hll">    <span class="k">if</span> <span class="n">quantize_base</span><span class="p">:</span>
</span><span class="hll">      <span class="bp">self</span><span class="o">.</span><span class="n">linear_weight</span> <span class="o">=</span> <span class="n">to_nf4</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear_weight</span><span class="p">)</span>
</span>    <span class="c1"># These are the new LoRA params. In general rank &lt;&lt; in_dim, out_dim</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">lora_a</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">lora_b</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="c1"># Rank and alpha are commonly-tuned hyperparameters</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">rank</span> <span class="o">=</span> <span class="n">rank</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span>

    <span class="c1"># Most implementations also include some dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span>

    <span class="c1"># The original params are frozen, and only LoRA params are trainable.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">lora_a</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">lora_b</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span>

  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
    <span class="c1"># frozen_out would be the output of the original model</span>
<span class="hll">    <span class="k">if</span> <span class="n">quantize_base</span><span class="p">:</span>
</span><span class="hll">      <span class="c1"># Call into torchao&#39;s linear_nf4 to run linear forward pass w/quantized weight.</span>
</span><span class="hll">      <span class="n">frozen_out</span>  <span class="o">=</span> <span class="n">linear_nf4</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
</span>    <span class="k">else</span><span class="p">:</span>
      <span class="n">frozen_out</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>

    <span class="c1"># lora_a projects inputs down to the much smaller self.rank,</span>
    <span class="c1"># then lora_b projects back up to the output dimension</span>
    <span class="n">lora_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lora_b</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lora_a</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>

    <span class="c1"># Finally, scale by the alpha parameter (normalized by rank)</span>
    <span class="c1"># and add to the original model&#39;s outputs</span>
    <span class="k">return</span> <span class="n">frozen_out</span> <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">rank</span><span class="p">)</span> <span class="o">*</span> <span class="n">lora_out</span>
</pre></div>
</div>
<p>As mentioned above, torchtune takes a dependency on torchao for some of the core components required for QLoRA. This includes the
<code class="docutils literal notranslate"><span class="pre">NF4Tensor</span></code>, as well as helpful utilities including <code class="docutils literal notranslate"><span class="pre">to_nf4</span></code> and <code class="docutils literal notranslate"><span class="pre">linear_nf4</span></code>.</p>
<p>The key changes on top of the LoRA layer are the usage of the <code class="docutils literal notranslate"><span class="pre">to_nf4</span></code> and <code class="docutils literal notranslate"><span class="pre">linear_nf4</span></code> APIs.</p>
<p><code class="docutils literal notranslate"><span class="pre">to_nf4</span></code> accepts an unquantized (bf16 or fp32) tensor and produces an <code class="docutils literal notranslate"><span class="pre">NF4</span></code> representation of the weight. See the <a class="reference external" href="https://github.com/pytorch-labs/ao/blob/c40358072f99b50cd7e58ec11e0e8d90440e3e25/torchao/dtypes/nf4tensor.py#L587">implementation</a> of <code class="docutils literal notranslate"><span class="pre">to_nf4</span></code> for more details.
<code class="docutils literal notranslate"><span class="pre">linear_nf4</span></code> handles the forward pass and autograd when running with quantized base model weights. It computes the forward pass as a regular
<code class="docutils literal notranslate"><span class="pre">F.linear</span></code> with the incoming activation and unquantized weight. The quantized weight is saved for backward, as opposed to the unquantized version of the weight, to avoid extra
memory usage due to storing higher precision variables to compute gradients in the backward pass. See <a class="reference external" href="https://github.com/pytorch-labs/ao/blob/main/torchao/dtypes/nf4tensor.py#L577">linear_nf4</a> for more details.</p>
</section>
</section>


             </article>
             
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="qat_finetune.html" class="btn btn-neutral float-right" title="Finetuning Llama3 with QAT" accesskey="n" rel="next">Next <img src="../_static/images/chevron-right-orange.svg" class="next-page"></a>
      
      
        <a href="lora_finetune.html" class="btn btn-neutral" title="Finetuning Llama2 with LoRA" accesskey="p" rel="prev"><img src="../_static/images/chevron-right-orange.svg" class="previous-page"> Previous</a>
      
    </div>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2023-present, torchtune Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">Finetuning Llama2 with QLoRA</a><ul>
<li><a class="reference internal" href="#what-is-qlora">What is QLoRA?</a></li>
<li><a class="reference internal" href="#using-qlora-to-save-memory">Using QLoRA to save memory</a></li>
<li><a class="reference internal" href="#using-qlora-in-torchtune">Using QLoRA in torchtune</a></li>
<li><a class="reference internal" href="#putting-it-all-together-qlora-finetune">Putting it all together: QLoRA finetune</a></li>
<li><a class="reference internal" href="#deep-dive-building-qlora-from-lora">Deep-dive: Building QLoRA from LoRA</a></li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
         <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
         <script src="../_static/jquery.js"></script>
         <script src="../_static/underscore.js"></script>
         <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="../_static/doctools.js"></script>
         <script src="../_static/clipboard.min.js"></script>
         <script src="../_static/copybutton.js"></script>
         <script src="../_static/design-tabs.js"></script>
     

  

  <script type="text/javascript" src="../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
<!-- Disabling "auto-collapsing" of sections on the left side bar. Replace script with commented out sections to reenable. -->
<!--  
<script script type="text/javascript">
    var collapsedSections = ['Introduction', 'Getting Started', 'Tutorials']
</script> -->

<script script type="text/javascript">
    var collapsedSections = []
</script>


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">Stay up to date</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          </ul>  
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch Podcasts</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
          </ul>
         </div>
        </div>
        
        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p>© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
          <a href="https://www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see <a href="https://www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
           <li class="resources-mobile-menu-title">
             <a>Learn</a>
           </li>
           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/get-started">Get Started</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials">Tutorials</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/beginner/basics/intro.html">Learn the Basics</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/recipes/recipes_index.html">PyTorch Recipes</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/beginner/introyt.html">Introduction to PyTorch - YouTube Series</a>
             </li>
           </ul>
           <li class="resources-mobile-menu-title">
             <a>Ecosystem</a>
           </li>
           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/ecosystem">Tools</a>
             </li>
             <li>
               <a href="https://pytorch.org/#community-module">Community</a>
             </li>
             <li>
               <a href="https://discuss.pytorch.org/">Forums</a>
             </li>
             <li>
               <a href="https://pytorch.org/resources">Developer Resources</a>
             </li>
             <li>
               <a href="https://pytorch.org/ecosystem/contributor-awards-2023">Contributor Awards - 2023</a>
             </li>
           </ul>

           <li class="resources-mobile-menu-title">
             <a>Edge</a>
           </li>

           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/edge">About PyTorch Edge</a>
             </li>
             
             <li>
               <a href="https://pytorch.org/executorch-overview">ExecuTorch</a>
             </li>
           </ul>

           <li class="resources-mobile-menu-title">
             <a>Docs</a>
           </li>

           <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/pytorch-domains">PyTorch Domains</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            <a>Blog & News</a>
          </li>
            
           <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/blog/">PyTorch Blog</a>
            </li>
            <li>
              <a href="https://pytorch.org/community-blog">Community Blog</a>
            </li>

            <li>
              <a href="https://pytorch.org/videos">Videos</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">Community Stories</a>
            </li>
            <li>
              <a href="https://pytorch.org/events">Events</a>
            </li>
          </ul>
          
          <li class="resources-mobile-menu-title">
            <a>About</a>
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
            </li>
            <li>
              <a href="https://pytorch.org/governing-board">Governing Board</a>
            </li>
          </ul>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>