{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5cd8586-8392-4c8e-99ef-0fb9adf8b6b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gemma's activation function should be approximate GeLU and not exact GeLU.\n",
      "Changing the activation function to `gelu_pytorch_tanh`.if you want to use the legacy `gelu`, edit the `model.config` to set `hidden_activation=gelu`   instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d94e5c9f94f14af3947f3cfbbca4553c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Write me a poem about Machine Learning.\n",
      "\n",
      "I’m not sure what you mean by “write me a poem about Machine Learning.”\n",
      "\n",
      "I’m not sure what you mean by “write me a poem about Machine Learning.”\n",
      "\n",
      "I’m not sure what you mean by “write\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b\", device_map=\"cuda\")\n",
    "\n",
    "input_text = \"Write me a poem about Machine Learning.\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**input_ids, max_new_tokens=50)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "492d2043-af1e-41d6-adff-6088fd73b00c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GemmaForCausalLM(\n",
       "  (model): GemmaModel(\n",
       "    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-17): 18 x GemmaDecoderLayer(\n",
       "        (self_attn): GemmaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): GemmaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): GemmaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): GemmaRMSNorm()\n",
       "        (post_attention_layernorm): GemmaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): GemmaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53dcf3bf-3916-4292-ae34-7e4f313fc4fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ -6.6963,   2.4422, -18.7727,  ...,  -7.9401,  -2.7512,  -6.6050],\n",
      "         [ -7.3636,  14.5272, -23.6110,  ...,  -9.1783,  -3.4405,  -7.2611],\n",
      "         [ -7.6963,  10.9584, -20.6943,  ...,  -9.9076,  -3.3036,  -7.5720],\n",
      "         ...,\n",
      "         [-14.3678,   3.1154, -12.6028,  ..., -11.5705, -11.9707, -14.4218],\n",
      "         [-22.8693,  -5.5922, -24.4688,  ..., -22.0343, -18.6676, -22.9308],\n",
      "         [-17.8728,   1.2903, -39.8689,  ..., -19.7219, -12.3213, -17.9414]]],\n",
      "       device='cuda:0', grad_fn=<UnsafeViewBackward0>)\n",
      "tensor([[ 1.4173,  1.9047, -3.1243,  ..., -0.5948, -6.0515, -5.6282],\n",
      "        [ 1.4173,  1.9047, -3.1243,  ..., -0.5948, -6.0515, -5.6282],\n",
      "        [ 1.4173,  1.9047, -3.1243,  ..., -0.5948, -6.0515, -5.6282],\n",
      "        ...,\n",
      "        [ 1.4173,  1.9047, -3.1243,  ..., -0.5948, -6.0515, -5.6282],\n",
      "        [ 1.4173,  1.9047, -3.1243,  ..., -0.5948, -6.0515, -5.6282],\n",
      "        [ 1.4173,  1.9047, -3.1243,  ..., -0.5948, -6.0515, -5.6282]],\n",
      "       device='cuda:0')\n",
      "tensor([[ 1.4173,  1.9047, -3.1243,  ..., -0.5948, -6.0515, -5.6282],\n",
      "        [ 1.4173,  1.9047, -3.1243,  ..., -0.5948, -6.0515, -5.6282],\n",
      "        [ 1.4173,  1.9047, -3.1243,  ..., -0.5948, -6.0515, -5.6282],\n",
      "        ...,\n",
      "        [ 1.4173,  1.9047, -3.1243,  ..., -0.5948, -6.0515, -5.6282],\n",
      "        [ 1.4173,  1.9047, -3.1243,  ..., -0.5948, -6.0515, -5.6282],\n",
      "        [ 1.4173,  1.9047, -3.1243,  ..., -0.5948, -6.0515, -5.6282]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(42)\n",
    "tokens = torch.randint(0, 256000, (1, 10))\n",
    "tokens = tokens.to(\"cuda\")\n",
    "output = model(tokens)\n",
    "print(output.logits)\n",
    "hf_output = output.logits\n",
    "loss = output.logits.sum()\n",
    "loss.backward()\n",
    "print(model.lm_head.weight.grad)\n",
    "print(model.model.embed_tokens.weight.grad)\n",
    "hf_grad = model.lm_head.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b254d57-ed3c-4311-98b3-3d60c52c8d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ -6.6963,   2.4422, -18.7727,  ...,  -7.9401,  -2.7512,  -6.6050],\n",
      "         [ -7.3636,  14.5272, -23.6109,  ...,  -9.1783,  -3.4404,  -7.2610],\n",
      "         [ -7.6963,  10.9585, -20.6943,  ...,  -9.9076,  -3.3036,  -7.5720],\n",
      "         ...,\n",
      "         [-14.3678,   3.1154, -12.6028,  ..., -11.5705, -11.9707, -14.4217],\n",
      "         [-22.8693,  -5.5922, -24.4688,  ..., -22.0343, -18.6676, -22.9308],\n",
      "         [-17.8728,   1.2904, -39.8689,  ..., -19.7219, -12.3213, -17.9414]]],\n",
      "       device='cuda:0', grad_fn=<UnsafeViewBackward0>)\n",
      "tensor([[ 1.4173,  1.9047, -3.1243,  ..., -0.5948, -6.0515, -5.6282],\n",
      "        [ 1.4173,  1.9047, -3.1243,  ..., -0.5948, -6.0515, -5.6282],\n",
      "        [ 1.4173,  1.9047, -3.1243,  ..., -0.5948, -6.0515, -5.6282],\n",
      "        ...,\n",
      "        [ 1.4173,  1.9047, -3.1243,  ..., -0.5948, -6.0515, -5.6282],\n",
      "        [ 1.4173,  1.9047, -3.1243,  ..., -0.5948, -6.0515, -5.6282],\n",
      "        [ 1.4173,  1.9047, -3.1243,  ..., -0.5948, -6.0515, -5.6282]],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'GemmaTransformerDecoder' object has no attribute 'output'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(model\u001b[38;5;241m.\u001b[39mtok_embeddings\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mgrad)\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mgrad)\n\u001b[1;32m     26\u001b[0m tt_grad \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mgrad\n",
      "File \u001b[0;32m~/.conda/envs/torchtune/lib/python3.10/site-packages/torch/nn/modules/module.py:1688\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1687\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1688\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GemmaTransformerDecoder' object has no attribute 'output'"
     ]
    }
   ],
   "source": [
    "from torchtune.models.gemma import gemma_2b\n",
    "from torchtune.utils import FullModelHFCheckpointer\n",
    "\n",
    "checkpointer = FullModelHFCheckpointer(\n",
    "  checkpoint_dir=\"/tmp/gemma/\",\n",
    "  checkpoint_files=[\n",
    "    \"model-00001-of-00002.safetensors\",\n",
    "    \"model-00002-of-00002.safetensors\",\n",
    "  ],\n",
    "  recipe_checkpoint=None,\n",
    "  output_dir=\"/tmp/gemma\",\n",
    "  model_type=\"GEMMA\",\n",
    ")\n",
    "sd = checkpointer.load_checkpoint()\n",
    "\n",
    "model = gemma_2b()\n",
    "model.load_state_dict(sd['model'])\n",
    "model = model.to(\"cuda\")\n",
    "output = model(tokens)\n",
    "print(output)\n",
    "tt_output = output\n",
    "loss = output.sum()\n",
    "loss.backward()\n",
    "print(model.tok_embeddings.weight.grad)\n",
    "print(model.output.weight.grad)\n",
    "tt_grad = model.output.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b742f557-62f8-4713-98f5-3be2bfa96a73",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Tensor-likes are not close!\n\nMismatched elements: 896814 / 2560000 (35.0%)\nGreatest absolute difference: 0.00025177001953125 at index (0, 7, 232983) (up to 1e-05 allowed)\nGreatest relative difference: 0.29729729890823364 at index (0, 1, 91390) (up to 1.3e-06 allowed)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtesting\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_close\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtt_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhf_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m torch\u001b[38;5;241m.\u001b[39mtesting\u001b[38;5;241m.\u001b[39massert_close(tt_grad, hf_grad)\n",
      "File \u001b[0;32m~/.conda/envs/torchtune/lib/python3.10/site-packages/torch/testing/_comparison.py:1529\u001b[0m, in \u001b[0;36massert_close\u001b[0;34m(actual, expected, allow_subclasses, rtol, atol, equal_nan, check_device, check_dtype, check_layout, check_stride, msg)\u001b[0m\n\u001b[1;32m   1507\u001b[0m error_metas \u001b[38;5;241m=\u001b[39m not_close_error_metas(\n\u001b[1;32m   1508\u001b[0m     actual,\n\u001b[1;32m   1509\u001b[0m     expected,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1524\u001b[0m     msg\u001b[38;5;241m=\u001b[39mmsg,\n\u001b[1;32m   1525\u001b[0m )\n\u001b[1;32m   1527\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_metas:\n\u001b[1;32m   1528\u001b[0m     \u001b[38;5;66;03m# TODO: compose all metas into one AssertionError\u001b[39;00m\n\u001b[0;32m-> 1529\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_metas[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto_error(msg)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Tensor-likes are not close!\n\nMismatched elements: 896814 / 2560000 (35.0%)\nGreatest absolute difference: 0.00025177001953125 at index (0, 7, 232983) (up to 1e-05 allowed)\nGreatest relative difference: 0.29729729890823364 at index (0, 1, 91390) (up to 1.3e-06 allowed)"
     ]
    }
   ],
   "source": [
    "torch.testing.assert_close(tt_output, hf_output)\n",
    "torch.testing.assert_close(tt_grad, hf_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25409837-6afb-40e0-aef6-6ef9be6828e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.9182e-05, device='cuda:0', grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt_output[0,1,91390]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22fce301-8acb-43dd-8a2f-5757cfb8d611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0001, device='cuda:0', grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_output[0,1,91390]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af164cdf-d68a-4cf3-9ad4-07d15e9a29de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ -6.6963,   2.4422, -18.7727,  ...,  -7.9401,  -2.7512,  -6.6050],\n",
      "         [ -7.3636,  14.5272, -23.6109,  ...,  -9.1783,  -3.4404,  -7.2610],\n",
      "         [ -7.6963,  10.9585, -20.6943,  ...,  -9.9076,  -3.3036,  -7.5720],\n",
      "         ...,\n",
      "         [-14.3678,   3.1154, -12.6028,  ..., -11.5705, -11.9707, -14.4217],\n",
      "         [-22.8693,  -5.5922, -24.4688,  ..., -22.0343, -18.6676, -22.9308],\n",
      "         [-17.8728,   1.2904, -39.8689,  ..., -19.7219, -12.3213, -17.9414]]],\n",
      "       device='cuda:0', grad_fn=<UnsafeViewBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from torchtune.models.gemma import gemma_2b\n",
    "from torchtune.utils import FullModelHFCheckpointer\n",
    "\n",
    "checkpointer = FullModelHFCheckpointer(\n",
    "  checkpoint_dir=\"/tmp/gemma/\",\n",
    "  checkpoint_files=[\n",
    "    \"model-00001-of-00002.safetensors\",\n",
    "    \"model-00002-of-00002.safetensors\",\n",
    "  ],\n",
    "  recipe_checkpoint=None,\n",
    "  output_dir=\"/tmp/gemma\",\n",
    "  model_type=\"GEMMA\",\n",
    ")\n",
    "sd = checkpointer.load_checkpoint()\n",
    "\n",
    "model = gemma_2b()\n",
    "model.load_state_dict(sd['model'])\n",
    "model = model.to(\"cuda\")\n",
    "output = model(tokens)\n",
    "print(output)\n",
    "loss = output.sum()\n",
    "loss.backward()\n",
    "print(model.tok_embeddings.weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ebfedf8d-7549-4ee2-9bb7-59c0e9b032b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "tokenizer/tokenizer.model",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgemma\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GemmaForCausalLM\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgemma\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_config_for_2b\n\u001b[0;32m----> 6\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mGemmaForCausalLM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_config_for_2b\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m model\n",
      "File \u001b[0;32m/data/users/rafiayub/torchtune/../gemma_pytorch/gemma/model.py:400\u001b[0m, in \u001b[0;36mGemmaForCausalLM.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    397\u001b[0m head_dim \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mhead_dim\n\u001b[1;32m    398\u001b[0m vocab_size \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mvocab_size\n\u001b[0;32m--> 400\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedder \u001b[38;5;241m=\u001b[39m Embedding(vocab_size, config\u001b[38;5;241m.\u001b[39mhidden_size, config\u001b[38;5;241m.\u001b[39mquant)\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m GemmaModel(config)\n",
      "File \u001b[0;32m/data/users/rafiayub/torchtune/../gemma_pytorch/gemma/tokenizer.py:24\u001b[0m, in \u001b[0;36mTokenizer.__init__\u001b[0;34m(self, model_path)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_path: Optional[\u001b[38;5;28mstr\u001b[39m]):\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m# Reload tokenizer.\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(model_path), model_path\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msp_model \u001b[38;5;241m=\u001b[39m SentencePieceProcessor(model_file\u001b[38;5;241m=\u001b[39mmodel_path)\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m# BOS / EOS token IDs.\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: tokenizer/tokenizer.model"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../gemma_pytorch\")\n",
    "from gemma.model import GemmaForCausalLM\n",
    "from gemma.config import get_config_for_2b\n",
    "\n",
    "model = GemmaForCausalLM(get_config_for_2b())\n",
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
