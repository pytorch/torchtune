# Config for FullFinetuneRecipe in full_finetune.py
#
# To launch, run the following command from root:
#    tune --nnodes 1 --nproc_per_node 1 full_finetune --config alpaca_llama2_full_finetune --override model_checkpoint=<your_checkpoint_dir> ...

# Tokenizer
tokenizer:
  _component_: torchtune.models.llama2.llama2_tokenizer
  path: /home/kartikayk/cpts/tokenizer.model

# Dataset
dataset:
  _component_: torchtune.datasets.AlpacaDataset
  train_on_input: True
seed: null
shuffle: True

# Model Arguments
model:
  _component_: torchtune.models.llama2.llama2_7b

# model_checkpoint:
#   checkpoint_dir: /home/kartikayk/cpts/llama-2-7b-hf
#   checkpoint_files: [pytorch_model-00001-of-00002.bin, pytorch_model-00002-of-00002.bin]
#   model_type: LLAMA2_7B
#   checkpoint_format: HF_FORMAT
# resume_from_checkpoint: False
model_checkpoint:
  checkpoint_dir: /tmp/alpaca-llama2-finetune/
  checkpoint_files: [model_0.pt]
  model_type: LLAMA2_7B
  checkpoint_format: TORCHTUNE_FORMAT
resume_from_checkpoint: True

# Fine-tuning arguments
batch_size: 2
epochs: 3
optimizer:
  _component_: torch.optim.SGD
  lr: 2e-5
loss:
  _component_: torch.nn.CrossEntropyLoss
max_steps_per_epoch: null
gradient_accumulation_steps: 1
log_every_n_steps: null
run_generation: null


# Distributed
device: cuda
dtype: fp32
enable_fsdp: True
enable_activation_checkpointing: True
cpu_offload: False

# Logging
metric_logger:
  _component_: torchtune.utils.metric_logging.DiskLogger
  log_dir: ${output_dir}
output_dir: /tmp/alpaca-llama2-finetune
