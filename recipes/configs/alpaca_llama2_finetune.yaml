# Dataset and Dataloader
dataset: alpaca
dataloader_seed: null
shuffle: True

# Model Arguments
model: llama2_7b
model_checkpoint: /tmp/llama2-7b-01112024
tokenizer: llama2_tokenizer
tokenizer_checkpoint: /tmp/tokenizer.model

# Fine-tuning arguments
batch_size: 2
lr: 2e-5
epochs: 3
optimizer: SGD
loss: CrossEntropyLoss
output_dir: /tmp/alpaca-llama2-finetune
device: cuda
dtype: bf16
fsdp: False
activation_checkpointing: False
