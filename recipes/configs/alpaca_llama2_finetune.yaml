# Dataset and Dataloader
dataset: alpaca
seed: null
shuffle: True

# Model Arguments
model: llama2_7b
model_checkpoint: /tmp/llama2-7b
tokenizer: llama2_tokenizer
tokenizer_checkpoint: /tmp/tokenizer.model

# Fine-tuning arguments
batch_size: 2
lr: 2e-5
epochs: 3
max_steps_per_epoch: 2
optimizer: SGD
loss: CrossEntropyLoss
output_dir: /tmp/alpaca-llama2-finetune
device: cuda
dtype: fp32
activation_checkpointing: False
cpu_offload: False
run_generation: 0
resume_from_previous_checkpoint: False
project: null

# Metrics arguments
metric_logger_type: disk
