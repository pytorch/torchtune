# Dataset and Dataloader
dataset: alpaca
dataloader_seed: null
shuffle: True

# Model Arguments
model: llama2_7b
model_checkpoint: /tmp/native_checkpoints/llama2-7b
tokenizer: llama2_tokenizer
tokenizer_checkpoint: /home/daniellepintz/llama/tokenizer.model

# Fine-tuning arguments
batch_size: 2
lr: 2e-5
epochs: 3
optimizer: SGD
loss: CrossEntropyLoss
output_dir: /tmp/alpaca-llama2-finetune
device: cuda
fsdp: False
activation_checkpointing: False
