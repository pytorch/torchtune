# Dataset and Dataloader
dataset: slimorca
seed: 10
shuffle: True

# Model Arguments
model: llama2_7b
model_checkpoint: /tmp/llama2-7b-01222024
tokenizer: llama2_tokenizer
tokenizer_checkpoint: /tmp/tokenizer.model

# Fine-tuning arguments
batch_size: 8
lr: 2e-5
epochs: 3
optimizer: SGD
loss: CrossEntropyLoss
output_dir: /tmp/alpaca-llama2-finetune
device: cuda
dtype: fp32
fsdp: False
activation_checkpointing: False
