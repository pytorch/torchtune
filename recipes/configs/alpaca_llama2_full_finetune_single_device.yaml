# Config for FullFinetuneRecipe in full_finetune.py
#
# To launch, run the following command from root:
#    tune --nnodes 1 --nproc_per_node 1 full_finetune --config alpaca_llama2_full_finetune model_checkpoint=<your_checkpoint_dir> ...

# Tokenizer
tokenizer:
  _component_: torchtune.models.llama2.llama2_tokenizer
  path: /home/rvarm1/local/dev/assets/tokenizer.model

# Dataset
dataset:
  _component_: torchtune.datasets.AlpacaDataset
  train_on_input: True
seed: null
shuffle: True

# Model Arguments
model:
  _component_: torchtune.models.llama2.llama2_7b

checkpointer:
  _component_: torchtune.utils.FullModelMetaCheckpointer
  checkpoint_dir: /home/rvarm1/local/dev/assets/
  checkpoint_files: [consolidated.00.pth]
  recipe_checkpoint: null
  output_dir: /tmp/llama2
  model_type: LLAMA2
resume_from_checkpoint: False

# Fine-tuning arguments
batch_size: 2
epochs: 3
optimizer:
  _component_: torch.optim.SGD
  lr: 2e-5
loss:
  _component_: torch.nn.CrossEntropyLoss
max_steps_per_epoch: null
gradient_accumulation_steps: 1
log_every_n_steps: null
run_generation: null


# Distributed
device: cuda
dtype: fp32
enable_fsdp: True
enable_activation_checkpointing: True
cpu_offload: False

# Logging
metric_logger:
  _component_: torchtune.utils.metric_logging.DiskLogger
  log_dir: ${output_dir}
output_dir: /tmp/alpaca-llama2-finetune
