# Model Arguments
model: lora_llama2_7b
model_checkpoint: /tmp/llama2_native 
lora_attn_modules: ['q_proj', 'v_proj']
lora_rank: 8
lora_alpha: 16
lora_checkpoint: null

# Tokenizer
tokenizer: llama2_tokenizer
tokenizer_checkpoint: /tmp/llama2/tokenizer.model

# Dataset and Sampler
dataset: alpaca
train_on_input: True
shuffle: True
batch_size: 2

# Optimizer and Scheduler
optimizer: AdamW
weight_decay: 0.01
lr: 3e-4
lr_scheduler: cosine_with_warmup
num_warmup_steps: 100
loss: CrossEntropyLoss

# Training
epochs: 1
resume_from_checkpoint: False

# Environment
device: cuda
dtype: fp32

# Logging
output_dir: /tmp/lora_finetune_output
