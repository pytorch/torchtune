#!/usr/bin/env python3

# Copyright (c) Facebook, Inc. and its affiliates.
# All rights reserved.
#
# This source code is licensed under the BSD-style license found in the
# LICENSE file in the root directory of this source tree.

"""
Docstring TODO
Tests TODO

import torch.distributed.run

parser = get_arg_parser()
"if 0 flags, then run without torch run
" if flags, then run with torch run

"""
import os
import sys
import runpy
from pathlib import Path
from torch.distributed.run import get_args_parser, run

import torchtune
from recipes import list_recipes


def is_distributed_args(args):
    total = len(sys.argv) - 1 # total args minus "tune"
    script_args = len(args.training_script_args) + 1 # script args + 1 for script name
    return total > script_args

if __name__ == "__main__":
    parser = get_args_parser()
    args = parser.parse_args()
    distributed_args = is_distributed_args(args)
    #TODO: add custom help message https://stackoverflow.com/questions/35847084/customize-argparse-help-message
    cmd = args.training_script
    if not cmd.endswith(".py"):
        pkg_path = str(Path(torchtune.__file__).parent.parent.absolute())
        if cmd == "recipe":
            assert not distributed_args, "You can't use distributed args with the recipe util"
            cmd = os.path.join(pkg_path, "scripts", "cli_utils", "recipe_utils.py")
        elif cmd == "config":
            assert not distributed_args, "You can't use distributed args with the config util"
            cmd = os.path.join(pkg_path, "scripts", "cli_utils", "config_utils.py")
        elif cmd in list_recipes():
            cmd = os.path.join(pkg_path, "recipes", f"{cmd}.py")
            args.training_script = cmd
        else:
            print("Unknown command, for a list of available commands, run with --help")

    if distributed_args:
        run(args)
    else:
        sys.argv = [cmd] + args.training_script_args
        runpy.run_path(cmd, run_name="__main__")
